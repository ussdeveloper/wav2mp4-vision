#!/usr/bin/env python3
"""
WAV to MP4 Converter with Audio Visualizer
Konwertuje pliki WAV do MP4 z wizualnym equalizrem
"""

import argparse
import sys
import os
import glob
import numpy as np
from scipy import signal
from scipy.io import wavfile
from moviepy import VideoClip, AudioFileClip
from PIL import Image, ImageDraw, ImageFilter, ImageChops, ImageFont
import wave
import tempfile
import shutil


class BackgroundManager:
    """Klasa do zarzƒÖdzania t≈Çem z obrazk√≥w"""
    
    def __init__(self, background_path, width, height, duration, crossfade_duration=2.0):
        """
        Inicjalizacja managera t≈Ça
        
        Args:
            background_path: ≈öcie≈ºka do pliku obrazka lub katalogu z obrazkami
            width: Szeroko≈õƒá wideo
            height: Wysoko≈õƒá wideo
            duration: Ca≈Çkowity czas trwania wideo
            crossfade_duration: Czas przej≈õcia miƒôdzy obrazkami (sekundy)
        """
        self.width = width
        self.height = height
        self.duration = duration
        self.crossfade_duration = crossfade_duration
        self.images = []
        self.use_ken_burns = False  # Efekt Ken Burns dla pojedynczego obrazka
        
        if background_path and os.path.exists(background_path):
            if os.path.isfile(background_path):
                # Pojedynczy plik
                self.images = [self._load_and_resize(background_path)]
            elif os.path.isdir(background_path):
                # Katalog z obrazkami
                patterns = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']
                files = []
                for pattern in patterns:
                    files.extend(glob.glob(os.path.join(background_path, pattern)))
                files.sort()
                
                if files:
                    self.images = [self._load_and_resize(f) for f in files]
        
        # Je≈õli brak obrazk√≥w, u≈ºyj czarnego t≈Ça
        if not self.images:
            black = Image.new('RGB', (width, height), color=(0, 0, 0))
            self.images = [black]
        
        # Oblicz czas wy≈õwietlania ka≈ºdego obrazka
        if len(self.images) > 1:
            self.time_per_image = duration / len(self.images)
        else:
            self.time_per_image = duration
            # Dla pojedynczego obrazka ZAWSZE u≈ºyj efektu Ken Burns (domy≈õlnie)
            if background_path and os.path.exists(background_path):
                self.use_ken_burns = True
                # Za≈Çaduj wiƒôkszy obrazek dla Ken Burns (+10% offset)
                if os.path.isfile(background_path):
                    self.ken_burns_img = self._load_ken_burns_image(background_path)
                elif os.path.isdir(background_path):
                    # Dla katalogu - znajd≈∫ pierwszy obrazek
                    patterns = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']
                    for pattern in patterns:
                        files = glob.glob(os.path.join(background_path, pattern))
                        if files:
                            files.sort()
                            self.ken_burns_img = self._load_ken_burns_image(files[0])
                            break
    
    def _load_and_resize(self, path):
        """Wczytaj i przeskaluj obrazek do rozmiaru wideo"""
        img = Image.open(path).convert('RGB')
        
        # Zachowaj proporcje, wype≈Çnij ca≈Çe t≈Ço
        img_ratio = img.width / img.height
        target_ratio = self.width / self.height
        
        if img_ratio > target_ratio:
            # Obrazek szerszy - skaluj po wysoko≈õci
            new_height = self.height
            new_width = int(new_height * img_ratio)
        else:
            # Obrazek wy≈ºszy - skaluj po szeroko≈õci
            new_width = self.width
            new_height = int(new_width / img_ratio)
        
        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # Wytnij ≈õrodek
        left = (new_width - self.width) // 2
        top = (new_height - self.height) // 2
        img = img.crop((left, top, left + self.width, top + self.height))
        
        return img
    
    def _load_ken_burns_image(self, path):
        """Wczytaj obrazek w wiƒôkszym rozmiarze dla efektu Ken Burns (zoom + pan)
        U≈ºywamy 2x upscale dla wiƒôkszej rozdzielczo≈õci = p≈Çynniejszy ruch bez skok√≥w"""
        img = Image.open(path).convert('RGB')
        
        # KROK 1: Upscale do 200% dla wiƒôkszej rozdzielczo≈õci (wiƒôcej pikseli = p≈Çynniejszy ruch)
        upscaled_w = img.width * 2
        upscaled_h = img.height * 2
        img = img.resize((upscaled_w, upscaled_h), Image.Resampling.LANCZOS)
        
        # KROK 2: Skaluj aby wype≈Çniƒá ekran w 2x + 10% offset dla p≈Çynnego przesuwania
        scale = 1.1
        target_width = int(self.width * 2 * scale)  # 2x bo obraz jest upscaled
        target_height = int(self.height * 2 * scale)
        
        img_ratio = img.width / img.height
        target_ratio = target_width / target_height
        
        # Zawsze wype≈Çnij ca≈Çy ekran (cover, nie contain)
        if img_ratio > target_ratio:
            # Obraz szerszy - skaluj po wysoko≈õci
            new_height = target_height
            new_width = int(new_height * img_ratio)
        else:
            # Obraz wy≈ºszy - skaluj po szeroko≈õci
            new_width = target_width
            new_height = int(new_width / img_ratio)
        
        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        return img
    
    def get_frame(self, t):
        """
        Pobierz klatkƒô t≈Ça dla czasu t z p≈Çynnym przej≈õciem
        
        Args:
            t: Czas w sekundach
            
        Returns:
            PIL Image
        """
        if len(self.images) == 1:
            # Efekt Ken Burns dla pojedynczego obrazka
            if self.use_ken_burns:
                return self._apply_ken_burns(t)
            return self.images[0].copy()
        
        # Kt√≥ry obrazek powinien byƒá wy≈õwietlany
        image_index = int(t / self.time_per_image)
        image_index = min(image_index, len(self.images) - 1)
        
        # Czas lokalny w ramach aktualnego obrazka
        local_t = t - (image_index * self.time_per_image)
        
        current_img = self.images[image_index]
        
        # Crossfade na ko≈Ñcu obrazka
        if image_index < len(self.images) - 1:
            fade_start = self.time_per_image - self.crossfade_duration
            if local_t >= fade_start:
                # Oblicz alpha dla przej≈õcia
                alpha = (local_t - fade_start) / self.crossfade_duration
                alpha = np.clip(alpha, 0, 1)
                
                next_img = self.images[image_index + 1]
                
                # Blend dw√≥ch obrazk√≥w
                blended = Image.blend(current_img, next_img, alpha)
                return blended
        
        return current_img.copy()
    
    def _apply_ken_burns(self, t):
        """Zastosuj p≈Çynny efekt Ken Burns (zoom do wype≈Çnienia +10% offset, potem p≈Çynne przesuwanie)
        Obraz jest w 2x rozdzielczo≈õci, wiƒôc cropujemy w 2x i skalujemy z powrotem"""
        if not hasattr(self, 'ken_burns_img'):
            return self.images[0].copy()
        
        # Progress od 0 do 1 przez ca≈Çy czas trwania (z easing dla p≈Çynno≈õci)
        progress = t / self.duration if self.duration > 0 else 0
        progress = np.clip(progress, 0, 1)
        
        # Smooth easing function (ease-in-out) dla ultra p≈Çynnego ruchu
        # U≈ºywamy sinusoidalnej krzywej dla naturalnego przyspieszenia/zwalniania
        smooth_progress = (1 - np.cos(progress * np.pi)) / 2
        
        # Wymiary obrazka w 2x rozdzielczo≈õci (ju≈º przeskalowanego do 2x * 110%)
        img_width = self.ken_burns_img.width
        img_height = self.ken_burns_img.height
        
        # Rozmiary docelowe w 2x (bo obraz jest upscaled)
        target_w = self.width * 2
        target_h = self.height * 2
        
        # Maksymalny dostƒôpny offset (10% z ka≈ºdej strony w przestrzeni 2x)
        max_offset_x = img_width - target_w
        max_offset_y = img_height - target_h
        
        # Szybka animacja z p≈Çynnym ruchem dziƒôki 2x upscale
        # U≈ºywamy 70% dostƒôpnego zakresu dla widocznego ruchu, ale upscale 2x daje p≈Çynno≈õƒá!
        # Przy 1920x1080 * 2 = 3840x2160 i 10% offset -> max_offset ‚âà 384px -> 70% z tego = ~269px ca≈Çkowity zakres
        # To da ~269 r√≥≈ºnych pozycji pikseli = ultra p≈Çynny ruch bez skok√≥w!
        # 480 klatek / 269px = ~1.8 klatki na pixel = oko nie widzi skok√≥w, tylko p≈Çynny ruch
        offset_x = int(smooth_progress * max_offset_x * 0.70)
        offset_y = int(smooth_progress * max_offset_y * 0.70)
        
        # Ogranicz do maksymalnego dostƒôpnego offsetu (zabezpieczenie)
        offset_x = min(offset_x, max_offset_x)
        offset_y = min(offset_y, max_offset_y)
        
        # Wytnij fragment obrazka w 2x rozdzielczo≈õci z p≈Çynnym przesuniƒôciem
        left = offset_x
        top = offset_y
        right = left + target_w
        bottom = top + target_h
        
        # Zabezpieczenie przed wyj≈õciem poza granice (nie powinno siƒô zdarzyƒá)
        if right > img_width:
            right = img_width
            left = right - target_w
        if bottom > img_height:
            bottom = img_height
            top = bottom - target_h
        
        # Crop w 2x rozdzielczo≈õci
        cropped = self.ken_burns_img.crop((left, top, right, bottom))
        
        # Skaluj z powrotem do docelowej rozdzielczo≈õci z wysokƒÖ jako≈õciƒÖ (Lanczos)
        cropped = cropped.resize((self.width, self.height), Image.Resampling.LANCZOS)
        
        # Dodaj delikatny efekt rozgrzanego powietrza (heat distortion)
        cropped = self._apply_heat_distortion(cropped, t)
        
        return cropped
    
    def _apply_heat_distortion(self, img, t):
        """Zastosuj delikatny efekt rozgrzanego powietrza (heat haze) - p≈Çynna animacja"""
        # Konwertuj do numpy array
        img_array = np.array(img, dtype=np.float32)
        height, width = img_array.shape[:2]
        
        # Parametry fali (bardzo subtelne) - p≈Çynna animacja co klatkƒô
        frequency = 0.4  # Czƒôstotliwo≈õƒá fali (ni≈ºsza = wiƒôksze fale)
        amplitude = 0.5  # Amplituda przesuniƒôcia (bardzo ma≈Ça - jeszcze subtelniej)
        speed = 0.1  # Bardzo wolna animacja dla p≈Çynno≈õci (wolniej)
        
        # Utw√≥rz siatkƒô wsp√≥≈Çrzƒôdnych
        x = np.arange(width)
        y = np.arange(height)
        
        # Fala sinusoidalna zmieniajƒÖca siƒô w czasie (pionowo)
        # R√≥≈ºne czƒôstotliwo≈õci dla x i y dla naturalnego efektu
        # P≈Çynna animacja co klatkƒô dla subtelnego efektu
        wave_y = amplitude * np.sin(2 * np.pi * frequency * (y / height) + t * speed)
        wave_x = amplitude * 0.7 * np.sin(2 * np.pi * frequency * 1.3 * (x / width) + t * speed * 0.8)
        
        # Utw√≥rz macierze przesuniƒôƒá
        shift_y = np.tile(wave_y.reshape(-1, 1), (1, width))
        shift_x = np.tile(wave_x, (height, 1))
        
        # Nowe wsp√≥≈Çrzƒôdne z przesuniƒôciem
        y_coords, x_coords = np.meshgrid(y, x, indexing='ij')
        y_new = np.clip(y_coords + shift_y, 0, height - 1).astype(np.int32)
        x_new = np.clip(x_coords + shift_x, 0, width - 1).astype(np.int32)
        
        # Zastosuj przesuniƒôcie
        distorted = img_array[y_new, x_new]
        
        # Konwertuj z powrotem do uint8
        distorted = np.clip(distorted, 0, 255).astype(np.uint8)
        
        return Image.fromarray(distorted)


class AudioVisualizer:
    """Klasa do analizy audio i generowania wizualizacji fal d≈∫wiƒôkowych"""
    
    def __init__(self, wav_file, num_bars=64):
        """
        Inicjalizacja wizualizatora
        
        Args:
            wav_file: ≈öcie≈ºka do pliku WAV
            num_bars: Liczba pr√≥bek dla fali (u≈ºywane dla sinusoidy)
        """
        self.wav_file = wav_file
        self.num_bars = num_bars
        
        # Wczytaj plik audio
        self.sample_rate, self.audio_data = wavfile.read(wav_file)
        
        # Zapisz stereo/mono info
        self.is_stereo = len(self.audio_data.shape) > 1
        
        if self.is_stereo:
            # Rozdziel kana≈Çy
            self.left_channel = self.audio_data[:, 0]
            self.right_channel = self.audio_data[:, 1]
        else:
            # Mono - u≈ºyj tego samego dla obu kana≈Ç√≥w
            self.left_channel = self.audio_data
            self.right_channel = self.audio_data
        
        # Normalizuj do float -1.0 do 1.0
        if self.left_channel.dtype == np.int16:
            self.left_channel = self.left_channel.astype(np.float32) / 32768.0
            self.right_channel = self.right_channel.astype(np.float32) / 32768.0
        elif self.left_channel.dtype == np.int32:
            self.left_channel = self.left_channel.astype(np.float32) / 2147483648.0
            self.right_channel = self.right_channel.astype(np.float32) / 2147483648.0
        
        self.duration = len(self.left_channel) / self.sample_rate
        
    def get_frequency_spectrum(self, start_time, duration=0.05):
        """
        Oblicz widmo czƒôstotliwo≈õci dla danego momentu
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            duration: D≈Çugo≈õƒá okna analizy w sekundach
            
        Returns:
            Array z amplitudami dla ka≈ºdego paska equalizera
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.audio_data))
        
        if start_sample >= len(self.audio_data):
            return np.zeros(self.num_bars)
        
        # Pobierz fragment audio
        audio_chunk = self.audio_data[start_sample:end_sample]
        
        if len(audio_chunk) == 0:
            return np.zeros(self.num_bars)
        
        # Zastosuj okno Hanninga dla lepszej analizy
        window = np.hanning(len(audio_chunk))
        audio_chunk = audio_chunk * window
        
        # FFT - Fast Fourier Transform
        fft = np.fft.fft(audio_chunk)
        fft_magnitude = np.abs(fft[:len(fft)//2])
        
        # Logarytmiczna skala czƒôstotliwo≈õci (bardziej naturalna dla ucha)
        freqs = np.fft.fftfreq(len(audio_chunk), 1/self.sample_rate)
        freqs = freqs[:len(freqs)//2]
        
        # Podziel czƒôstotliwo≈õci na pasy (logarytmicznie)
        min_freq = 20  # Hz
        max_freq = min(20000, self.sample_rate / 2)  # Hz
        
        freq_bands = np.logspace(np.log10(min_freq), np.log10(max_freq), self.num_bars + 1)
        
        bar_heights = np.zeros(self.num_bars)
        
        for i in range(self.num_bars):
            # Znajd≈∫ indeksy dla danego pasma
            mask = (freqs >= freq_bands[i]) & (freqs < freq_bands[i + 1])
            if np.any(mask):
                # ≈örednia amplituda w pa≈õmie
                bar_heights[i] = np.mean(fft_magnitude[mask])
        
        # Normalizuj i zastosuj skalƒô logarytmicznƒÖ dla lepszego efektu wizualnego
        bar_heights = np.log10(bar_heights + 1)
        max_height = np.max(bar_heights) if np.max(bar_heights) > 0 else 1
        bar_heights = bar_heights / max_height
        
        # Wyg≈Çad≈∫ (aby uniknƒÖƒá zbyt gwa≈Çtownych zmian)
        bar_heights = np.clip(bar_heights, 0, 1)
        
        return bar_heights
    
    def get_waveform_data(self, start_time, window_duration=0.05, num_points=200):
        """
        Pobierz dane fali d≈∫wiƒôkowej dla obu kana≈Ç√≥w
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            window_duration: D≈Çugo≈õƒá okna w sekundach
            num_points: Liczba punkt√≥w do zwr√≥cenia
            
        Returns:
            Tuple (left_wave, right_wave) - arrays z amplitudami
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(window_duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.left_channel))
        
        if start_sample >= len(self.left_channel):
            return np.zeros(num_points), np.zeros(num_points)
        
        # Pobierz fragmenty
        left_chunk = self.left_channel[start_sample:end_sample]
        right_chunk = self.right_channel[start_sample:end_sample]
        
        if len(left_chunk) == 0:
            return np.zeros(num_points), np.zeros(num_points)
        
        # Resample do num_points
        if len(left_chunk) > num_points:
            # Downsampling
            indices = np.linspace(0, len(left_chunk) - 1, num_points).astype(int)
            left_wave = left_chunk[indices]
            right_wave = right_chunk[indices]
        else:
            # Interpolacja je≈õli za ma≈Ço pr√≥bek
            x_old = np.arange(len(left_chunk))
            x_new = np.linspace(0, len(left_chunk) - 1, num_points)
            left_wave = np.interp(x_new, x_old, left_chunk)
            right_wave = np.interp(x_new, x_old, right_chunk)
        
        return left_wave, right_wave
    
    def extract_vocal_frequencies(self, start_time, window_duration=0.05, num_points=200):
        """
        Ekstraktuj czƒôstotliwo≈õci wokalne (300Hz-3000Hz) z sygna≈Çu stereo
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            window_duration: D≈Çugo≈õƒá okna w sekundach
            num_points: Liczba punkt√≥w do zwr√≥cenia
            
        Returns:
            Array z amplitudami pasma wokalnego
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(window_duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.left_channel))
        
        if start_sample >= len(self.left_channel):
            return np.zeros(num_points)
        
        # Pobierz fragmenty i u≈õrednij stereo
        left_chunk = self.left_channel[start_sample:end_sample]
        right_chunk = self.right_channel[start_sample:end_sample]
        mono_chunk = (left_chunk + right_chunk) / 2
        
        if len(mono_chunk) == 0:
            return np.zeros(num_points)
        
        # Zastosuj filtr pasmowy dla czƒôstotliwo≈õci wokalnych (300Hz-3000Hz)
        nyquist = self.sample_rate / 2
        low_freq = 300 / nyquist
        high_freq = min(3000 / nyquist, 0.99)
        
        # Projektuj filtr Butterwortha
        from scipy.signal import butter, filtfilt
        b, a = butter(4, [low_freq, high_freq], btype='band')
        
        try:
            vocal_chunk = filtfilt(b, a, mono_chunk)
        except:
            # Je≈õli filtrowanie siƒô nie uda, u≈ºyj surowego sygna≈Çu
            vocal_chunk = mono_chunk
        
        # Resample do num_points
        if len(vocal_chunk) > num_points:
            indices = np.linspace(0, len(vocal_chunk) - 1, num_points).astype(int)
            vocal_wave = vocal_chunk[indices]
        else:
            x_old = np.arange(len(vocal_chunk))
            x_new = np.linspace(0, len(vocal_chunk) - 1, num_points)
            vocal_wave = np.interp(x_new, x_old, vocal_chunk)
        
        return vocal_wave


class VideoGenerator:
    """Klasa do generowania wideo z wizualizacjƒÖ"""
    
    def __init__(self, width, height, fps=30, bars=64, waveform_style='waveform', 
                 left_color=(0, 100, 255), right_color=(0, 255, 100), opacity=0.9,
                 vocal_color=(255, 0, 100), text=None, text_opacity=0.8,
                 watermark=None, watermark_x=10, watermark_y=10, add_flares=True,
                 flare_duration=500, screen_flash_intensity=0.3):
        """
        Inicjalizacja generatora wideo
        
        Args:
            width: Szeroko≈õƒá wideo
            height: Wysoko≈õƒá wideo
            fps: Klatki na sekundƒô
            bars: Liczba pask√≥w equalizera (lub punkt√≥w dla waveform)
            waveform_style: 'bars' dla equalizera, 'waveform' dla sinusoid
            left_color: Kolor lewego kana≈Çu (R, G, B)
            right_color: Kolor prawego kana≈Çu (R, G, B)
            vocal_color: Kolor wokalu (R, G, B)
            opacity: Przezroczysto≈õƒá wizualizacji (0.0-1.0)
            text: Tekst do wy≈õwietlenia (None = brak)
            text_opacity: Przezroczysto≈õƒá tekstu (0.0-1.0)
            watermark: ≈öcie≈ºka do pliku znaku wodnego (None = brak)
            watermark_x: Pozycja X znaku wodnego w % (0-100)
            watermark_y: Pozycja Y znaku wodnego w % (0-100)
            add_flares: Dodaj flary na szczytach amplitudy (True/False)
        """
        self.width = width
        self.height = height
        self.fps = fps
        self.bars = bars
        self.waveform_style = waveform_style
        self.left_color = left_color
        self.right_color = right_color
        self.vocal_color = vocal_color
        self.opacity = opacity
        self.text = text
        self.text_opacity = text_opacity
        self.watermark = watermark
        self.watermark_x = watermark_x
        self.watermark_y = watermark_y
        self.add_flares = add_flares
        
        # Historia dla efektu reverb (trailing)
        self.wave_history = []
        
        # Historia flar (aktywne flary z czasem ≈ºycia)
        self.active_flares = []  # Lista: (x, y, color, birth_time, is_record)
        self.flare_lifetime = flare_duration / 1000.0  # Konwersja ms na sekundy
        self.current_time = 0  # Aktualny czas w sekundach
        
        # System rekord√≥w amplitudy (resetuje siƒô co kilka sekund)
        self.amplitude_record = 0.0  # Maksymalna amplituda od ostatniego resetu
        self.record_reset_interval = 3.0  # Reset co 3 sekundy
        self.last_record_reset = 0.0  # Czas ostatniego resetu
        
        # Absolutny rekord amplitudy (nigdy siƒô nie resetuje)
        self.absolute_record = 0.0  # Najwy≈ºsza amplituda w ca≈Çym utworze
        self.screen_flash_intensity = screen_flash_intensity  # Intensywno≈õƒá flasha (0.0-1.0)
        self.active_flashes = []  # Lista: (birth_time, intensity, x, y) - flashe rozchodzƒÖce siƒô od punktu
        self.flash_duration = 0.3  # 300ms trwania flasha (d≈Çu≈ºej dla efektu rozchodzenia)
        
        # Za≈Çaduj font
        self.font = self._load_font()
        
        # Za≈Çaduj watermark je≈õli jest podany
        self.watermark_img = None
        if watermark and os.path.exists(watermark):
            self.watermark_img = self._load_watermark(watermark)
        
        # Kolory gradientu (od niebieskiego przez zielony do czerwonego)
        if waveform_style == 'bars':
            self.colors = self._generate_gradient_colors()
    
    def _load_font(self):
        """Za≈Çaduj font Arial lub Roboto"""
        font_size = int(self.height * 0.015)  # 1.5% wysoko≈õci ekranu (50% mniejszy)
        
        # Spr√≥buj r√≥≈ºne fonty
        font_names = [
            'arial.ttf',
            'Arial.ttf',
            'roboto.ttf',
            'Roboto-Regular.ttf',
            'segoeui.ttf',  # Windows fallback
        ]
        
        for font_name in font_names:
            try:
                # Spr√≥buj za≈Çadowaƒá z systemowych czcionek Windows
                font_path = f"C:\\Windows\\Fonts\\{font_name}"
                if os.path.exists(font_path):
                    return ImageFont.truetype(font_path, font_size)
            except:
                continue
        
        # Fallback do domy≈õlnego fontu
        try:
            return ImageFont.truetype("arial.ttf", font_size)
        except:
            return ImageFont.load_default()
    
    def _load_watermark(self, watermark_path):
        """Za≈Çaduj i przeskaluj watermark"""
        try:
            watermark = Image.open(watermark_path).convert('RGBA')
            
            # Przeskaluj watermark do max 15% szeroko≈õci ekranu
            max_width = int(self.width * 0.15)
            ratio = max_width / watermark.width
            new_width = max_width
            new_height = int(watermark.height * ratio)
            
            watermark = watermark.resize((new_width, new_height), Image.Resampling.LANCZOS)
            return watermark
        except Exception as e:
            print(f"‚ö†Ô∏è Nie mo≈ºna za≈Çadowaƒá watermark: {e}")
            return None
    
    def _draw_watermark(self, img):
        """Rysuj watermark na obrazie"""
        if self.watermark_img is None:
            return
        
        # Oblicz pozycjƒô w pikselach na podstawie % od lewej g√≥rnej
        x = int((self.watermark_x / 100) * self.width)
        y = int((self.watermark_y / 100) * self.height)
        
        # Na≈Ç√≥≈º watermark
        img.paste(self.watermark_img, (x, y), self.watermark_img)
        
    def _generate_gradient_colors(self):
        """Generuj gradient kolor√≥w dla pask√≥w"""
        colors = []
        for i in range(self.bars):
            # Gradient: niebieski -> cyan -> zielony -> ≈º√≥≈Çty -> czerwony
            ratio = i / self.bars
            
            if ratio < 0.25:
                # Niebieski do cyan
                r = 0
                g = int((ratio / 0.25) * 255)
                b = 255
            elif ratio < 0.5:
                # Cyan do zielonego
                r = 0
                g = 255
                b = int((1 - (ratio - 0.25) / 0.25) * 255)
            elif ratio < 0.75:
                # Zielony do ≈º√≥≈Çtego
                r = int(((ratio - 0.5) / 0.25) * 255)
                g = 255
                b = 0
            else:
                # ≈ª√≥≈Çty do czerwonego
                r = 255
                g = int((1 - (ratio - 0.75) / 0.25) * 255)
                b = 0
            
            colors.append((r, g, b))
        
        return colors
    
    def create_frame(self, bar_heights=None, smoothed_heights=None, 
                    left_wave=None, right_wave=None, vocal_wave=None, background=None, time=0):
        """
        Utw√≥rz pojedynczƒÖ klatkƒô z wizualizacjƒÖ
        
        Args:
            bar_heights: Array z wysoko≈õciami pask√≥w (0-1) - dla stylu 'bars'
            smoothed_heights: Poprzednie wysoko≈õci dla wyg≈Çadzania
            left_wave: Array z amplitudami lewego kana≈Çu - dla stylu 'waveform'
            right_wave: Array z amplitudami prawego kana≈Çu - dla stylu 'waveform'
            vocal_wave: Array z amplitudami wokalu - dla stylu 'waveform'
            background: PIL Image z t≈Çem (opcjonalne)
            time: Aktualny czas w sekundach (dla animacji flar)
            
        Returns:
            numpy array z wizualizacjƒÖ
        """
        # Zapisz aktualny czas dla animacji flar
        self.current_time = time
        
        # U≈ºyj t≈Ça lub utw√≥rz czarne
        if background is not None:
            img = background.copy()
        else:
            img = Image.new('RGB', (self.width, self.height), color=(0, 0, 0))
        
        # Utw√≥rz warstwƒô z wizualizacjƒÖ
        overlay = Image.new('RGBA', (self.width, self.height), color=(0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        if self.waveform_style == 'waveform' and left_wave is not None and right_wave is not None:
            # Rysuj sinusoidy
            self._draw_waveforms(draw, left_wave, right_wave, vocal_wave)
        elif self.waveform_style == 'bars' and bar_heights is not None:
            # Rysuj equalizera (stary spos√≥b)
            self._draw_bars(draw, bar_heights, smoothed_heights)
        
        # NIE stosuj blur - ostre linie dla lepszej jako≈õci
        # overlay = overlay.filter(ImageFilter.GaussianBlur(radius=3))
        
        # Na≈Ç√≥≈º wizualizacjƒô na t≈Ço
        img = img.convert('RGBA')
        img = Image.alpha_composite(img, overlay)
        
        # Dodaj flash na ca≈Çy ekran dla absolutnych rekord√≥w (przed tekstem)
        if self.screen_flash_intensity > 0:
            self._draw_screen_flash(img)
        
        # Dodaj tekst je≈õli jest ustawiony
        if self.text:
            self._draw_text(img)
        
        # Dodaj watermark je≈õli jest ustawiony
        if self.watermark:
            self._draw_watermark(img)
        
        img = img.convert('RGB')
        
        return np.array(img)
    
    def _draw_screen_flash(self, img):
        """Rysuj flash rozchodzƒÖcy siƒô od miejsca absolutnego rekordu amplitudy (jak ripple)"""
        # Usu≈Ñ wygas≈Çe flashe
        self.active_flashes = [
            flash for flash in self.active_flashes
            if (self.current_time - flash[0]) < self.flash_duration
        ]
        
        # Rysuj wszystkie aktywne flashe jako ripple wysokiej rozdzielczo≈õci
        draw = ImageDraw.Draw(img)
        for birth_time, intensity, x, y in self.active_flashes:
            age = self.current_time - birth_time
            progress = age / self.flash_duration  # 0 do 1
            
            # Flash rozchodzi siƒô jak ripple - wiele cienkich koncentrycznych okrƒôg√≥w
            max_distance = max(self.width, self.height) * 1.5
            
            # Wysoka rozdzielczo≈õƒá - wiƒôcej cie≈Ñszych fal (15-20 fal)
            num_ripples = 18
            for i in range(num_ripples):
                # Ka≈ºda fala zaczyna siƒô z mniejszym op√≥≈∫nieniem (bardziej gƒôste)
                wave_delay = i * 0.05  # 5% op√≥≈∫nienia miƒôdzy falami (by≈Ço 15%)
                wave_progress = (progress - wave_delay) / (1.0 - wave_delay)
                
                if wave_progress > 0 and wave_progress < 1:
                    # Promie≈Ñ dla tej fali
                    wave_radius = wave_progress * max_distance
                    
                    # Dynamiczne opacity - nieliniowe zanikanie (p≈Çynniejsze)
                    # U≈ºyj krzywej ease-out dla naturalnego zanikania
                    fade_curve = 1 - (wave_progress ** 1.5)  # Szybsze zanikanie na ko≈Ñcu
                    
                    # Dodatkowy gradient dla odleg≈Çych fal
                    distance_fade = 1.0 - (i / num_ripples) * 0.7  # Ostatnie fale: 30% intensywno≈õci
                    
                    # Po≈ÇƒÖcz intensity, krzywƒÖ zanikania i gradient odleg≈Ço≈õci
                    current_opacity = int(255 * intensity * fade_curve * distance_fade)
                    
                    if current_opacity > 8 and wave_radius > 3:
                        # Cienkie pier≈õcienie (sta≈Ça szeroko≈õƒá 2-3px dla subtelno≈õci)
                        ring_width = 2 if wave_progress > 0.3 else 3
                        
                        # Kolor bia≈Çy z dynamicznym opacity
                        color_ring = (255, 255, 255, current_opacity)
                        
                        # Rysuj cienki pier≈õcie≈Ñ (outline)
                        draw.ellipse(
                            [x - wave_radius, y - wave_radius,
                             x + wave_radius, y + wave_radius],
                            outline=color_ring,
                            width=ring_width
                        )
    
    def _draw_text(self, img):
        """Rysuj tekst w prawym dolnym rogu"""
        draw = ImageDraw.Draw(img)
        
        # Konwertuj tekst na wielkie litery (CAPS)
        text_caps = self.text.upper()
        
        # Oblicz pozycjƒô (2% marginesu)
        margin_x = int(self.width * 0.02)
        margin_y = int(self.height * 0.02)
        
        # Pobierz rozmiar tekstu
        try:
            bbox = draw.textbbox((0, 0), text_caps, font=self.font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
        except:
            # Fallback dla starszych wersji Pillow
            text_width, text_height = draw.textsize(text_caps, font=self.font)
        
        # Pozycja: prawy dolny r√≥g z marginesem
        x = self.width - text_width - margin_x
        y = self.height - text_height - margin_y
        
        # Rysuj tekst z cieniem dla lepszej czytelno≈õci
        shadow_offset = 2
        shadow_color = (0, 0, 0, int(255 * self.text_opacity))
        text_color = (255, 255, 255, int(255 * self.text_opacity))
        
        # Cie≈Ñ
        draw.text((x + shadow_offset, y + shadow_offset), text_caps, 
                 font=self.font, fill=shadow_color)
        # Tekst
        draw.text((x, y), text_caps, font=self.font, fill=text_color)
    
    def _draw_waveforms(self, draw, left_wave, right_wave, vocal_wave=None):
        """
        Rysuj trzy sinusoidy (≈º√≥≈Çta dla lewego, zielona dla prawego, czerwona dla wokalu) na ≈õrodku ekranu z efektem reverb
        
        Args:
            draw: ImageDraw object
            left_wave: Array z amplitudami lewego kana≈Çu
            right_wave: Array z amplitudami prawego kana≈Çu
            vocal_wave: Array z amplitudami wokalu (opcjonalne)
        """
        # Parametry
        center_y = self.height / 2
        amplitude_scale = self.height * 0.35  # 35% wysoko≈õci dla amplitudy
        line_width = 1  # Ultra cienka linia - neon
        vocal_line_width = 1  # Wokal te≈º 1px (neon)
        glow_width = 8  # Szeroko≈õƒá glow (blur)
        
        # Lewy kana≈Ç - ≈º√≥≈Çty (na ≈õrodku)
        points_left = []
        for i, amp in enumerate(left_wave):
            x = (i / len(left_wave)) * self.width
            y = center_y + (amp * amplitude_scale)
            points_left.append((x, y))
        
        # Prawy kana≈Ç - zielony (na ≈õrodku - na sobie)
        points_right = []
        for i, amp in enumerate(right_wave):
            x = (i / len(right_wave)) * self.width
            y = center_y + (amp * amplitude_scale)
            points_right.append((x, y))
        
        # Wokal - czerwony (na ≈õrodku, je≈õli jest)
        points_vocal = []
        if vocal_wave is not None:
            for i, amp in enumerate(vocal_wave):
                x = (i / len(vocal_wave)) * self.width
                y = center_y + (amp * amplitude_scale * 0.8)  # Trochƒô mniejsza amplituda
                points_vocal.append((x, y))
        
        # Dodaj do historii dla efektu reverb
        self.wave_history.append((points_left, points_right, points_vocal))
        
        # Zachowaj tylko ostatnie N klatek dla efektu trailing
        max_history = 5
        if len(self.wave_history) > max_history:
            self.wave_history.pop(0)
        
        # Rysuj trailing (starsze fale z mniejszƒÖ opacity)
        for idx, history_item in enumerate(self.wave_history[:-1]):
            # Rozpakowuj historiƒô (mo≈ºe byƒá 2 lub 3 elementy)
            if len(history_item) == 3:
                old_left, old_right, old_vocal = history_item
            else:
                old_left, old_right = history_item
                old_vocal = []
            
            # Oblicz opacity dla starszych klatek (efekt zanikania)
            age_factor = (idx + 1) / len(self.wave_history)
            trail_opacity = 0.4 * age_factor * 0.5  # Reverb z opacity 0.4
            
            # Rysuj trailing lewego kana≈Çu z glow
            if len(old_left) > 1:
                for i in range(len(old_left) - 1):
                    # Glow (blur effect) - rysuj grubszƒÖ liniƒô z mniejszƒÖ opacity
                    glow_opacity = int(255 * trail_opacity * 0.3)
                    color_glow = self.left_color + (glow_opacity,)
                    draw.line([old_left[i], old_left[i + 1]], 
                             fill=color_glow, width=glow_width)
                    # Ostra linia 1px na wierzchu
                    color_with_alpha = self.left_color + (int(255 * trail_opacity),)
                    draw.line([old_left[i], old_left[i + 1]], 
                             fill=color_with_alpha, width=line_width)
            
            # Rysuj trailing prawego kana≈Çu z glow
            if len(old_right) > 1:
                for i in range(len(old_right) - 1):
                    # Glow (blur effect)
                    glow_opacity = int(255 * trail_opacity * 0.3)
                    color_glow = self.right_color + (glow_opacity,)
                    draw.line([old_right[i], old_right[i + 1]], 
                             fill=color_glow, width=glow_width)
                    # Ostra linia 1px na wierzchu
                    color_with_alpha = self.right_color + (int(255 * trail_opacity),)
                    draw.line([old_right[i], old_right[i + 1]], 
                             fill=color_with_alpha, width=line_width)
            
            # Rysuj trailing wokalu z glow
            if len(old_vocal) > 1:
                for i in range(len(old_vocal) - 1):
                    # Glow (blur effect)
                    glow_opacity = int(255 * trail_opacity * 0.3)
                    color_glow = self.vocal_color + (glow_opacity,)
                    draw.line([old_vocal[i], old_vocal[i + 1]], 
                             fill=color_glow, width=glow_width)
                    # Ostra linia 1px na wierzchu
                    color_with_alpha = self.vocal_color + (int(255 * trail_opacity),)
                    draw.line([old_vocal[i], old_vocal[i + 1]], 
                             fill=color_with_alpha, width=line_width)
        
        # Rysuj aktualnƒÖ falƒô lewego kana≈Çu (pe≈Çna opacity) z glow
        if len(points_left) > 1:
            for i in range(len(points_left) - 1):
                # Glow (blur effect) - grubsza linia z mniejszƒÖ opacity
                glow_opacity = int(255 * self.opacity * 0.4)
                color_glow = self.left_color + (glow_opacity,)
                draw.line([points_left[i], points_left[i + 1]], 
                         fill=color_glow, width=glow_width)
                # Ostra linia 1px na wierzchu (neon)
                color_with_alpha = self.left_color + (int(255 * self.opacity),)
                draw.line([points_left[i], points_left[i + 1]], 
                         fill=color_with_alpha, width=line_width)
        
        # Rysuj aktualnƒÖ falƒô prawego kana≈Çu z glow
        if len(points_right) > 1:
            for i in range(len(points_right) - 1):
                # Glow (blur effect)
                glow_opacity = int(255 * self.opacity * 0.4)
                color_glow = self.right_color + (glow_opacity,)
                draw.line([points_right[i], points_right[i + 1]], 
                         fill=color_glow, width=glow_width)
                # Ostra linia 1px na wierzchu (neon)
                color_with_alpha = self.right_color + (int(255 * self.opacity),)
                draw.line([points_right[i], points_right[i + 1]], 
                         fill=color_with_alpha, width=line_width)
        
        # Rysuj aktualnƒÖ falƒô wokalu (na wierzchu) z glow - czerwony neon
        if len(points_vocal) > 1:
            for i in range(len(points_vocal) - 1):
                # Glow (blur effect) - mocniejszy dla wokalu
                glow_opacity = int(255 * self.opacity * 0.5)
                color_glow = self.vocal_color + (glow_opacity,)
                draw.line([points_vocal[i], points_vocal[i + 1]], 
                         fill=color_glow, width=glow_width)
                # Ostra linia 1px na wierzchu (czerwony neon)
                color_with_alpha = self.vocal_color + (int(255 * self.opacity),)
                draw.line([points_vocal[i], points_vocal[i + 1]], 
                         fill=color_with_alpha, width=vocal_line_width)
        
        # Rysuj flary na szczytach amplitudy (opcjonalnie)
        if self.add_flares:
            self._draw_flares(draw, points_left, points_right, points_vocal, center_y)
    
    def _draw_flares(self, draw, points_left, points_right, points_vocal, center_y):
        """Rysuj animowane flary (ripple effect) na szczytach amplitudy"""
        
        # Znajd≈∫ lokalne maksima i dodaj nowe flary
        self._detect_and_add_flares(points_left, center_y)
        self._detect_and_add_flares(points_right, center_y)
        if points_vocal:
            self._detect_and_add_flares(points_vocal, center_y, is_vocal=True)
        
        # Usu≈Ñ wygas≈Çe flary (starsze ni≈º lifetime)
        self.active_flares = [
            flare for flare in self.active_flares 
            if (self.current_time - flare[3]) < self.flare_lifetime
        ]
        
        # Rysuj wszystkie aktywne flary z animacjƒÖ ripple
        for flare_data in self.active_flares:
            # Rozpakowuj dane flary (obs≈Çuguj stary i nowy format)
            if len(flare_data) == 5:
                x, y, color, birth_time, is_record = flare_data
            else:
                x, y, color, birth_time = flare_data
                is_record = False
            
            age = self.current_time - birth_time
            progress = age / self.flare_lifetime  # 0 do 1
            
            # Parametry intensywno≈õci dla rekord√≥w amplitudy
            if is_record:
                max_radius = 35  # Wiƒôkszy promie≈Ñ dla rekord√≥w
                base_opacity = 250  # Bardziej intensywny
                line_width_base = 5  # Grubsza linia
                inner_glow_duration = 0.5  # D≈Çu≈ºszy blask
            else:
                max_radius = 20  # Normalny promie≈Ñ
                base_opacity = 200
                line_width_base = 3
                inner_glow_duration = 0.3
            
            # Efekt ripple: okrƒÖg powiƒôksza siƒô i zanika
            current_radius = progress * max_radius
            
            # Opacity zanika liniowo
            current_opacity = int(base_opacity * (1 - progress))
            
            # Rysuj ripple jako rozszerzajƒÖcy siƒô okrƒÖg
            if current_radius > 1:
                # Grubsza linia na poczƒÖtku, cie≈Ñsza na ko≈Ñcu
                line_width = max(1, int(line_width_base * (1 - progress)))
                
                color_with_alpha = color + (current_opacity,)
                
                # Rysuj okrƒÖg (outline)
                draw.ellipse(
                    [x - current_radius, y - current_radius, 
                     x + current_radius, y + current_radius],
                    outline=color_with_alpha,
                    width=line_width
                )
                
                # Dodaj wewnƒôtrzny blask (mniejszy okrƒÖg)
                if progress < inner_glow_duration:
                    inner_radius = current_radius * 0.5
                    inner_opacity = int(current_opacity * 0.6)
                    inner_color = color + (inner_opacity,)
                    draw.ellipse(
                        [x - inner_radius, y - inner_radius,
                         x + inner_radius, y + inner_radius],
                        fill=inner_color
                    )
                    
                    # Dla rekord√≥w - dodaj jeszcze intensywniejszy ≈õrodek
                    if is_record and progress < 0.2:
                        core_radius = current_radius * 0.2
                        core_opacity = int(255 * (1 - progress / 0.2))
                        # Bia≈Çy ≈õrodek dla maksymalnej jasno≈õci
                        core_color = (255, 255, 255, core_opacity)
                        draw.ellipse(
                            [x - core_radius, y - core_radius,
                             x + core_radius, y + core_radius],
                            fill=core_color
                        )
    
    def _detect_and_add_flares(self, points, center_y, is_vocal=False):
        """Wykryj lokalne maksima i dodaj nowe flary"""
        if not points or len(points) < 3:
            return
        
        # Reset rekord√≥w amplitudy co kilka sekund
        if self.current_time - self.last_record_reset > self.record_reset_interval:
            self.amplitude_record = 0.0
            self.last_record_reset = self.current_time
        
        for i in range(1, len(points) - 1):
            x, y = points[i]
            y_prev = points[i-1][1]
            y_next = points[i+1][1]
            
            # Szukaj lokalnych maksim√≥w (szczyt√≥w)
            distance_from_center = abs(y - center_y)
            threshold = 15 if is_vocal else 25
            
            if distance_from_center > threshold:
                if (y < y_prev and y < y_next) or (y > y_prev and y > y_next):
                    # Sprawd≈∫ czy flara ju≈º nie istnieje w tym miejscu (unikaj duplikat√≥w)
                    exists = any(
                        abs(flare[0] - x) < 10 and abs(flare[1] - y) < 10
                        for flare in self.active_flares
                    )
                    
                    if not exists:
                        # Sprawd≈∫ czy to nowy rekord amplitudy w oknie 3s
                        is_record = distance_from_center > self.amplitude_record
                        if is_record:
                            self.amplitude_record = distance_from_center
                        
                        # Sprawd≈∫ czy to absolutny rekord (najwy≈ºszy ever)
                        if distance_from_center > self.absolute_record:
                            self.absolute_record = distance_from_center
                            # Dodaj flash rozchodzƒÖcy siƒô od miejsca rekordu
                            if self.screen_flash_intensity > 0:
                                # Intensywno≈õƒá zale≈ºy od tego jak du≈ºy jest skok
                                flash_intensity = self.screen_flash_intensity
                                # Zapisz pozycjƒô (x, y) gdzie wystƒÖpi≈Ç rekord
                                self.active_flashes.append((self.current_time, flash_intensity, x, y))
                        
                        # Kolor flary zale≈ºy od pozycji (czƒôstotliwo≈õci)
                        if is_vocal:
                            flare_color = (255, 100, 50)  # Wokal - czerwony/pomara≈Ñczowy
                        else:
                            ratio = i / len(points)
                            flare_color = self._get_flare_color(ratio)
                        
                        # Dodaj nowƒÖ flarƒô z informacjƒÖ czy to rekord
                        self.active_flares.append((x, y, flare_color, self.current_time, is_record))
    
    def _get_flare_color(self, ratio):
        """Pobierz kolor flary na podstawie pozycji (czƒôstotliwo≈õci)"""
        # Gradient kolor√≥w: niski -> ≈õredni -> wysoki
        # Niebieski (bas) -> Cyan -> Zielony -> ≈ª√≥≈Çty -> Pomara≈Ñczowy -> Czerwony (wysoki)
        
        if ratio < 0.2:
            # Niskie czƒôstotliwo≈õci - niebieski/cyan
            r = int(ratio / 0.2 * 100)
            g = int(ratio / 0.2 * 200)
            b = 255
        elif ratio < 0.4:
            # ≈örednie-niskie - cyan/zielony
            local_ratio = (ratio - 0.2) / 0.2
            r = 0
            g = 200 + int(local_ratio * 55)
            b = 255 - int(local_ratio * 155)
        elif ratio < 0.6:
            # ≈örednie - zielony/≈º√≥≈Çty
            local_ratio = (ratio - 0.4) / 0.2
            r = int(local_ratio * 255)
            g = 255
            b = 100 - int(local_ratio * 100)
        elif ratio < 0.8:
            # ≈örednie-wysokie - ≈º√≥≈Çty/pomara≈Ñczowy
            local_ratio = (ratio - 0.6) / 0.2
            r = 255
            g = 255 - int(local_ratio * 100)
            b = 0
        else:
            # Wysokie czƒôstotliwo≈õci - pomara≈Ñczowy/czerwony
            local_ratio = (ratio - 0.8) / 0.2
            r = 255
            g = 155 - int(local_ratio * 155)
            b = 0
        
        return (r, g, b)
    
    def _draw_bars(self, draw, bar_heights, smoothed_heights):
        """
        Rysuj equalizera (paski)
        
        Args:
            draw: ImageDraw object  
            bar_heights: Array z wysoko≈õciami pask√≥w (0-1)
            smoothed_heights: Poprzednie wysoko≈õci dla wyg≈Çadzania
        """
        # Wyg≈Çad≈∫ przej≈õcia miƒôdzy klatkami
        if smoothed_heights is not None:
            bar_heights = 0.7 * bar_heights + 0.3 * smoothed_heights
        
        # Parametry pask√≥w
        bar_width = self.width / self.bars
        max_bar_height = self.height * 0.8
        base_y = self.height * 0.9
        
        # Rysuj paski
        for i, height in enumerate(bar_heights):
            x = i * bar_width
            bar_h = height * max_bar_height
            y = base_y - bar_h
            
            # G≈Ç√≥wny pasek z alpha
            color = self.colors[i]
            color_with_alpha = color + (int(255 * 0.6),)
            
            # Konwertuj do int dla rectangle
            x1, y1 = int(x + 1), int(y)
            x2, y2 = int(x + bar_width - 1), int(base_y)
            
            # Rysuj prostokƒÖt
            for yi in range(y1, y2):
                draw.line([x1, yi, x2, yi], fill=color_with_alpha)


def process_batch(batch_dir, args):
    """
    Przetwarzanie wsadowe katalog√≥w
    
    Args:
        batch_dir: Katalog zawierajƒÖcy podkatalogi z plikami WAV i obrazkami
        args: Argumenty z parsera
    """
    if not os.path.isdir(batch_dir):
        print(f"‚ùå {batch_dir} nie jest katalogiem")
        return
    
    # Parse kolor√≥w
    left_color = tuple(map(int, args.left_color.split(',')))
    right_color = tuple(map(int, args.right_color.split(',')))
    
    print(f"üîÑ Tryb batch: przetwarzam katalog {batch_dir}")
    print("=" * 70)
    
    # Szukaj podkatalog√≥w
    subdirs = [d for d in os.listdir(batch_dir) 
               if os.path.isdir(os.path.join(batch_dir, d))]
    
    if not subdirs:
        print(f"‚ùå Brak podkatalog√≥w w {batch_dir}")
        return
    
    total = len(subdirs)
    for idx, subdir in enumerate(subdirs, 1):
        subdir_path = os.path.join(batch_dir, subdir)
        print(f"\n[{idx}/{total}] üìÅ Przetwarzam: {subdir}")
        print("-" * 70)
        
        # Znajd≈∫ plik WAV
        wav_files = glob.glob(os.path.join(subdir_path, "*.wav")) + \
                   glob.glob(os.path.join(subdir_path, "*.WAV"))
        
        if not wav_files:
            print(f"‚ö†Ô∏è  Brak pliku WAV w {subdir}, pomijam...")
            continue
        
        wav_file = wav_files[0]  # U≈ºyj pierwszego znalezionego
        
        # Sprawd≈∫ czy sƒÖ obrazki w podkatalogu
        image_files = []
        for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
            image_files.extend(glob.glob(os.path.join(subdir_path, ext)))
        
        # U≈ºyj katalogu jako t≈Ço je≈õli sƒÖ obrazki, w przeciwnym razie None
        background = subdir_path if image_files else None
        
        # Wygeneruj nazwƒô pliku wyj≈õciowego
        base_name = os.path.splitext(os.path.basename(wav_file))[0]
        output_file = os.path.join(subdir_path, f"{base_name}.mp4")
        
        try:
            create_video_from_wav(
                wav_file,
                output_file,
                resolution=args.resolution,
                audio_bitrate=args.audio_bitrate,
                fps=args.fps,
                bars=args.bars,
                background=background,
                waveform_style=args.style,
                left_color=left_color,
                right_color=right_color,
                opacity=args.opacity,
                text=args.text,
                text_opacity=args.text_opacity,
                watermark=args.watermark,
                watermark_x=args.watermark_x,
                watermark_y=args.watermark_y,
                test_length=args.test_length,
                add_flares=not args.no_flares,
                flare_duration=args.flare_duration,
                screen_flash_intensity=args.screen_flash
            )
            print(f"‚úÖ Uko≈Ñczono: {output_file}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd dla {subdir}: {e}")
            continue
    
    print("\n" + "=" * 70)
    print(f"üéâ Batch processing zako≈Ñczony! Przetworzono {total} katalog√≥w.")


def create_video_from_wav(input_wav, output_mp4, resolution="1920x1080", 
                         audio_bitrate="320k", fps=30, bars=750,
                         background=None, waveform_style='waveform',
                         left_color=(255, 255, 0), right_color=(0, 255, 0),
                         opacity=0.9, text=None, text_opacity=0.8,
                         watermark=None, watermark_x=10, watermark_y=10,
                         test_length=None, add_flares=True, flare_duration=500,
                         screen_flash_intensity=0.0):
    """
    G≈Ç√≥wna funkcja konwertujƒÖca WAV do MP4 z wizualizacjƒÖ
    
    Args:
        input_wav: ≈öcie≈ºka do pliku WAV
        output_mp4: ≈öcie≈ºka do pliku MP4 wyj≈õciowego
        resolution: Rozdzielczo≈õƒá w formacie "WIDTHxHEIGHT"
        audio_bitrate: Bitrate audio (np. "320k", "192k")
        fps: Klatki na sekundƒô
        bars: Liczba pask√≥w equalizera/punkt√≥w fali
        background: ≈öcie≈ºka do obrazka/katalogu z t≈Çem
        waveform_style: 'waveform' dla sinusoid, 'bars' dla equalizera
        left_color: Kolor lewego kana≈Çu (R, G, B)
        right_color: Kolor prawego kana≈Çu (R, G, B)
        opacity: Przezroczysto≈õƒá wizualizacji (0.0-1.0)
        text: Tekst do wy≈õwietlenia (None = brak)
        text_opacity: Przezroczysto≈õƒá tekstu (0.0-1.0)
    """
    print(f"üìÅ Wczytujƒô plik: {input_wav}")
    
    # Je≈õli output_mp4 nie ma ≈õcie≈ºki, zapisz w lokalizacji input
    if not os.path.dirname(output_mp4):
        input_dir = os.path.dirname(os.path.abspath(input_wav))
        output_mp4 = os.path.join(input_dir, output_mp4)
    
    # Parse resolution
    width, height = map(int, resolution.lower().split('x'))
    print(f"üì∫ Rozdzielczo≈õƒá: {width}x{height}")
    print(f"üéµ Bitrate audio: {audio_bitrate}")
    print(f"üé¨ FPS: {fps}")
    print(f"üìä Styl wizualizacji: {waveform_style}")
    
    # Auto-wykrywanie obrazka z katalogu utworu (je≈õli nie podano background)
    if not background:
        input_dir = os.path.dirname(os.path.abspath(input_wav))
        # Szukaj obrazk√≥w (png, jpg, jpeg) w katalogu utworu
        image_exts = ['.png', '.jpg', '.jpeg']
        for file in os.listdir(input_dir):
            if any(file.lower().endswith(ext) for ext in image_exts):
                background = os.path.join(input_dir, file)
                print(f"üñºÔ∏è  Auto: Znaleziono t≈Ço {file} w katalogu utworu")
                break
    
    if background:
        print(f"üñºÔ∏è  T≈Ço: {background}")
    
    # Tryb testowy - obetnij plik WAV najpierw
    wav_to_process = input_wav
    temp_wav = None
    
    if test_length is not None:
        print(f"‚ö° TRYB TESTOWY: Przycinam plik do {test_length}%")
        
        # Wczytaj WAV
        sample_rate, audio_data = wavfile.read(input_wav)
        original_duration = len(audio_data) / sample_rate
        target_duration = original_duration * (test_length / 100)
        target_samples = int(sample_rate * target_duration)
        
        # Obetnij audio
        cut_audio = audio_data[:target_samples]
        
        # Zapisz do tymczasowego pliku w katalogu tmp
        tmp_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'tmp')
        os.makedirs(tmp_dir, exist_ok=True)
        
        temp_wav = os.path.join(tmp_dir, f"temp_test_{os.path.basename(input_wav)}")
        wavfile.write(temp_wav, sample_rate, cut_audio)
        
        wav_to_process = temp_wav
        print(f"‚ö° TRYB TESTOWY: {test_length}% pliku ({target_duration:.2f}s z {original_duration:.2f}s)")
    
    # Inicjalizuj analizator audio
    visualizer = AudioVisualizer(wav_to_process, num_bars=bars)
    
    print(f"‚è±Ô∏è  D≈Çugo≈õƒá: {visualizer.duration:.2f} sekund")
    print(f"üîä Format: {'Stereo' if visualizer.is_stereo else 'Mono'}")
    
    # Inicjalizuj generator wideo
    video_gen = VideoGenerator(width, height, fps, bars, waveform_style, 
                              left_color, right_color, opacity,
                              vocal_color=(255, 50, 50), text=text, text_opacity=text_opacity,
                              watermark=watermark, watermark_x=watermark_x, watermark_y=watermark_y,
                              add_flares=add_flares, flare_duration=flare_duration,
                              screen_flash_intensity=screen_flash_intensity)
    
    # Inicjalizuj manager t≈Ça
    bg_manager = BackgroundManager(background, width, height, visualizer.duration)
    
    # Stan dla wyg≈Çadzania animacji
    previous_heights = np.zeros(bars)
    previous_left_wave = np.zeros(bars)
    previous_right_wave = np.zeros(bars)
    previous_vocal_wave = np.zeros(bars)
    
    def make_frame(t):
        """Funkcja generujƒÖca klatkƒô dla czasu t"""
        nonlocal previous_heights, previous_left_wave, previous_right_wave, previous_vocal_wave
        
        # Pobierz t≈Ço
        bg = bg_manager.get_frame(t)
        
        if waveform_style == 'waveform':
            # Pobierz dane fali dla obu kana≈Ç√≥w
            left_wave, right_wave = visualizer.get_waveform_data(t, num_points=bars)
            
            # Wyg≈Çadzanie
            left_wave = 0.7 * left_wave + 0.3 * previous_left_wave
            right_wave = 0.7 * right_wave + 0.3 * previous_right_wave
            
            # Ekstraktuj wokal
            vocal_wave = visualizer.extract_vocal_frequencies(t, num_points=bars)
            vocal_wave = 0.7 * vocal_wave + 0.3 * previous_vocal_wave
            
            # Utw√≥rz klatkƒô
            frame = video_gen.create_frame(
                left_wave=left_wave,
                right_wave=right_wave,
                vocal_wave=vocal_wave,
                background=bg,
                time=t
            )
            
            # Zapamiƒôtaj
            previous_left_wave = left_wave
            previous_right_wave = right_wave
            previous_vocal_wave = vocal_wave
        else:
            # Styl equalizera (bars)
            bar_heights = visualizer.get_frequency_spectrum(t)
            
            # Utw√≥rz klatkƒô
            frame = video_gen.create_frame(
                bar_heights=bar_heights,
                smoothed_heights=previous_heights,
                background=bg,
                time=t
            )
            
            # Zapamiƒôtaj
            previous_heights = bar_heights
        
        return frame
    
    print("üé® Generujƒô wizualizacjƒô...")
    
    # Utw√≥rz klip wideo
    video_clip = VideoClip(make_frame, duration=visualizer.duration)
    video_clip = video_clip.with_fps(fps)
    
    # Wczytaj audio
    audio_clip = AudioFileClip(wav_to_process)
    
    # Po≈ÇƒÖcz wideo z audio
    final_clip = video_clip.with_audio(audio_clip)
    
    print(f"üíæ Zapisujƒô do: {output_mp4}")
    
    # Zapisz jako MP4 z dobrƒÖ jako≈õciƒÖ
    # USU≈É WSZYSTKIE METADANE z pliku wej≈õciowego (tylko tre≈õƒá audio/wideo)
    final_clip.write_videofile(
        output_mp4,
        codec='libx264',
        audio_codec='aac',
        audio_bitrate=audio_bitrate,
        fps=fps,
        preset='slow',  # Lepsza jako≈õƒá, wolniejsze kodowanie
        bitrate='8000k',  # Wysokie bitrate wideo dla dobrej jako≈õci
        # NIE kopiuj metadanych - usu≈Ñ wszystkie metadane z pliku wej≈õciowego
        ffmpeg_params=['-map_metadata', '-1']
    )
    
    print("‚úÖ Gotowe!")
    print(f"üì¶ Plik zapisany: {output_mp4}")
    
    # Usu≈Ñ tymczasowy plik WAV
    if temp_wav and os.path.exists(temp_wav):
        try:
            os.remove(temp_wav)
            print("üóëÔ∏è  Usuniƒôto tymczasowy plik")
        except:
            pass


def main():
    """G≈Ç√≥wna funkcja programu"""
    parser = argparse.ArgumentParser(
        description='Konwertuj WAV do MP4 z wizualnƒÖ wizualizacjƒÖ audio',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Przyk≈Çady u≈ºycia:
  # Podstawowe z tekstem
  python main.py song.wav output.mp4 --text "My Song Title"
  
  # Z t≈Çem i znakiem wodnym
  python main.py song.wav output.mp4 --background photo.jpg --watermark logo.png
  
  # Test pierwszych 10%% (szybkie sprawdzenie)
  python main.py song.wav test.mp4 --test-length 10
  
  # Pe≈Çna konfiguracja
  python main.py song.wav output.mp4 --background ./images/ --text "Song 2025" --watermark logo.png --watermark-x 5 --watermark-y 5
  
  # Tryb batch
  python main.py batch-folder dummy.mp4 --batch
        """
    )
    
    parser.add_argument('input', help='Plik WAV wej≈õciowy')
    parser.add_argument('output', help='Plik MP4 wyj≈õciowy')
    parser.add_argument('--resolution', default='1920x1080',
                       help='Rozdzielczo≈õƒá wideo (domy≈õlnie: 1920x1080)')
    parser.add_argument('--audio-bitrate', default='320k',
                       help='Bitrate audio (domy≈õlnie: 320k)')
    parser.add_argument('--fps', type=int, default=30,
                       help='Klatki na sekundƒô (domy≈õlnie: 30)')
    parser.add_argument('--bars', type=int, default=750,
                       help='Liczba punkt√≥w wizualizacji (domy≈õlnie: 750 dla wy≈ºszej rozdzielczo≈õci)')
    parser.add_argument('--background', default=None,
                       help='≈öcie≈ºka do obrazka lub katalogu z obrazkami dla t≈Ça')
    parser.add_argument('--style', default='waveform', choices=['waveform', 'bars'],
                       help='Styl wizualizacji: waveform (sinusoidy) lub bars (equalizera)')
    parser.add_argument('--left-color', default='255,255,0',
                       help='Kolor lewego kana≈Çu w formacie R,G,B (domy≈õlnie: 255,255,0 - ≈º√≥≈Çty)')
    parser.add_argument('--right-color', default='0,255,0',
                       help='Kolor prawego kana≈Çu w formacie R,G,B (domy≈õlnie: 0,255,0 - zielony)')
    parser.add_argument('--opacity', type=float, default=0.9,
                       help='Przezroczysto≈õƒá wizualizacji 0.0-1.0 (domy≈õlnie: 0.9)')
    parser.add_argument('--text', default=None,
                       help='Tekst do wy≈õwietlenia w prawym dolnym rogu (zawsze CAPS)')
    parser.add_argument('--text-opacity', type=float, default=0.8,
                       help='Przezroczysto≈õƒá tekstu 0.0-1.0 (domy≈õlnie: 0.8)')
    parser.add_argument('--watermark', default=None,
                       help='≈öcie≈ºka do pliku znaku wodnego (PNG/JPG z alpha channel)')
    parser.add_argument('--watermark-x', type=float, default=10,
                       help='Pozycja X znaku wodnego w %% od lewej (domy≈õlnie: 10)')
    parser.add_argument('--watermark-y', type=float, default=10,
                       help='Pozycja Y znaku wodnego w %% od g√≥ry (domy≈õlnie: 10)')
    parser.add_argument('--test-length', type=float, default=None,
                       help='Renderuj tylko X%% pliku dla szybkich test√≥w (np. 10 = pierwsze 10%%)')
    parser.add_argument('--no-flares', action='store_true',
                       help='Wy≈ÇƒÖcz kolorowe flary na szczytach amplitudy (domy≈õlnie: w≈ÇƒÖczone)')
    parser.add_argument('--flare-duration', type=int, default=500,
                       help='Czas ≈ºycia flary w milisekundach (domy≈õlnie: 500ms)')
    parser.add_argument('--screen-flash', type=float, default=0.0,
                       help='Intensywno≈õƒá flasha rozchodzƒÖcego siƒô od rekord√≥w (0.06-0.9, 0=wy≈ÇƒÖczone, domy≈õlnie: 0.0 - wy≈ÇƒÖczone)')
    parser.add_argument('--batch', action='store_true',
                       help='Tryb batch - przetwarzaj katalogi z podkatalogami zawierajƒÖcymi WAV+obrazki')
    
    args = parser.parse_args()
    
    # Parse kolor√≥w
    try:
        left_color = tuple(map(int, args.left_color.split(',')))
        right_color = tuple(map(int, args.right_color.split(',')))
        
        if len(left_color) != 3 or len(right_color) != 3:
            raise ValueError("Kolory muszƒÖ mieƒá 3 sk≈Çadowe (R,G,B)")
    except ValueError as e:
        print(f"‚ùå B≈ÇƒÖd parsowania kolor√≥w: {e}", file=sys.stderr)
        sys.exit(1)
    
    try:
        if args.batch:
            # Tryb batch processing
            process_batch(args.input, args)
        else:
            # Pojedynczy plik
            create_video_from_wav(
                args.input,
                args.output,
                resolution=args.resolution,
                audio_bitrate=args.audio_bitrate,
                fps=args.fps,
                bars=args.bars,
                background=args.background,
                waveform_style=args.style,
                left_color=left_color,
                right_color=right_color,
                opacity=args.opacity,
                text=args.text,
                text_opacity=args.text_opacity,
                watermark=args.watermark,
                watermark_x=args.watermark_x,
                watermark_y=args.watermark_y,
                test_length=args.test_length,
                add_flares=not args.no_flares,
                flare_duration=args.flare_duration,
                screen_flash_intensity=args.screen_flash
            )
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
