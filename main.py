#!/usr/bin/env python3
"""
WAV to MP4 Converter with Audio Visualizer
Konwertuje pliki WAV do MP4 z wizualnym equalizrem
"""

import argparse
import sys
import os
import glob
import numpy as np
from scipy import signal
from scipy.io import wavfile
from moviepy import VideoClip, AudioFileClip
from PIL import Image, ImageDraw, ImageFilter, ImageChops, ImageFont
import wave


class BackgroundManager:
    """Klasa do zarzƒÖdzania t≈Çem z obrazk√≥w"""
    
    def __init__(self, background_path, width, height, duration, crossfade_duration=2.0):
        """
        Inicjalizacja managera t≈Ça
        
        Args:
            background_path: ≈öcie≈ºka do pliku obrazka lub katalogu z obrazkami
            width: Szeroko≈õƒá wideo
            height: Wysoko≈õƒá wideo
            duration: Ca≈Çkowity czas trwania wideo
            crossfade_duration: Czas przej≈õcia miƒôdzy obrazkami (sekundy)
        """
        self.width = width
        self.height = height
        self.duration = duration
        self.crossfade_duration = crossfade_duration
        self.images = []
        
        if background_path and os.path.exists(background_path):
            if os.path.isfile(background_path):
                # Pojedynczy plik
                self.images = [self._load_and_resize(background_path)]
            elif os.path.isdir(background_path):
                # Katalog z obrazkami
                patterns = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']
                files = []
                for pattern in patterns:
                    files.extend(glob.glob(os.path.join(background_path, pattern)))
                files.sort()
                
                if files:
                    self.images = [self._load_and_resize(f) for f in files]
        
        # Je≈õli brak obrazk√≥w, u≈ºyj czarnego t≈Ça
        if not self.images:
            black = Image.new('RGB', (width, height), color=(0, 0, 0))
            self.images = [black]
        
        # Oblicz czas wy≈õwietlania ka≈ºdego obrazka
        if len(self.images) > 1:
            self.time_per_image = duration / len(self.images)
        else:
            self.time_per_image = duration
    
    def _load_and_resize(self, path):
        """Wczytaj i przeskaluj obrazek do rozmiaru wideo"""
        img = Image.open(path).convert('RGB')
        
        # Zachowaj proporcje, wype≈Çnij ca≈Çe t≈Ço
        img_ratio = img.width / img.height
        target_ratio = self.width / self.height
        
        if img_ratio > target_ratio:
            # Obrazek szerszy - skaluj po wysoko≈õci
            new_height = self.height
            new_width = int(new_height * img_ratio)
        else:
            # Obrazek wy≈ºszy - skaluj po szeroko≈õci
            new_width = self.width
            new_height = int(new_width / img_ratio)
        
        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # Wytnij ≈õrodek
        left = (new_width - self.width) // 2
        top = (new_height - self.height) // 2
        img = img.crop((left, top, left + self.width, top + self.height))
        
        return img
    
    def get_frame(self, t):
        """
        Pobierz klatkƒô t≈Ça dla czasu t z p≈Çynnym przej≈õciem
        
        Args:
            t: Czas w sekundach
            
        Returns:
            PIL Image
        """
        if len(self.images) == 1:
            return self.images[0].copy()
        
        # Kt√≥ry obrazek powinien byƒá wy≈õwietlany
        image_index = int(t / self.time_per_image)
        image_index = min(image_index, len(self.images) - 1)
        
        # Czas lokalny w ramach aktualnego obrazka
        local_t = t - (image_index * self.time_per_image)
        
        current_img = self.images[image_index]
        
        # Crossfade na ko≈Ñcu obrazka
        if image_index < len(self.images) - 1:
            fade_start = self.time_per_image - self.crossfade_duration
            if local_t >= fade_start:
                # Oblicz alpha dla przej≈õcia
                alpha = (local_t - fade_start) / self.crossfade_duration
                alpha = np.clip(alpha, 0, 1)
                
                next_img = self.images[image_index + 1]
                
                # Blend dw√≥ch obrazk√≥w
                blended = Image.blend(current_img, next_img, alpha)
                return blended
        
        return current_img.copy()


class AudioVisualizer:
    """Klasa do analizy audio i generowania wizualizacji fal d≈∫wiƒôkowych"""
    
    def __init__(self, wav_file, num_bars=64):
        """
        Inicjalizacja wizualizatora
        
        Args:
            wav_file: ≈öcie≈ºka do pliku WAV
            num_bars: Liczba pr√≥bek dla fali (u≈ºywane dla sinusoidy)
        """
        self.wav_file = wav_file
        self.num_bars = num_bars
        
        # Wczytaj plik audio
        self.sample_rate, self.audio_data = wavfile.read(wav_file)
        
        # Zapisz stereo/mono info
        self.is_stereo = len(self.audio_data.shape) > 1
        
        if self.is_stereo:
            # Rozdziel kana≈Çy
            self.left_channel = self.audio_data[:, 0]
            self.right_channel = self.audio_data[:, 1]
        else:
            # Mono - u≈ºyj tego samego dla obu kana≈Ç√≥w
            self.left_channel = self.audio_data
            self.right_channel = self.audio_data
        
        # Normalizuj do float -1.0 do 1.0
        if self.left_channel.dtype == np.int16:
            self.left_channel = self.left_channel.astype(np.float32) / 32768.0
            self.right_channel = self.right_channel.astype(np.float32) / 32768.0
        elif self.left_channel.dtype == np.int32:
            self.left_channel = self.left_channel.astype(np.float32) / 2147483648.0
            self.right_channel = self.right_channel.astype(np.float32) / 2147483648.0
        
        self.duration = len(self.left_channel) / self.sample_rate
        
    def get_frequency_spectrum(self, start_time, duration=0.05):
        """
        Oblicz widmo czƒôstotliwo≈õci dla danego momentu
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            duration: D≈Çugo≈õƒá okna analizy w sekundach
            
        Returns:
            Array z amplitudami dla ka≈ºdego paska equalizera
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.audio_data))
        
        if start_sample >= len(self.audio_data):
            return np.zeros(self.num_bars)
        
        # Pobierz fragment audio
        audio_chunk = self.audio_data[start_sample:end_sample]
        
        if len(audio_chunk) == 0:
            return np.zeros(self.num_bars)
        
        # Zastosuj okno Hanninga dla lepszej analizy
        window = np.hanning(len(audio_chunk))
        audio_chunk = audio_chunk * window
        
        # FFT - Fast Fourier Transform
        fft = np.fft.fft(audio_chunk)
        fft_magnitude = np.abs(fft[:len(fft)//2])
        
        # Logarytmiczna skala czƒôstotliwo≈õci (bardziej naturalna dla ucha)
        freqs = np.fft.fftfreq(len(audio_chunk), 1/self.sample_rate)
        freqs = freqs[:len(freqs)//2]
        
        # Podziel czƒôstotliwo≈õci na pasy (logarytmicznie)
        min_freq = 20  # Hz
        max_freq = min(20000, self.sample_rate / 2)  # Hz
        
        freq_bands = np.logspace(np.log10(min_freq), np.log10(max_freq), self.num_bars + 1)
        
        bar_heights = np.zeros(self.num_bars)
        
        for i in range(self.num_bars):
            # Znajd≈∫ indeksy dla danego pasma
            mask = (freqs >= freq_bands[i]) & (freqs < freq_bands[i + 1])
            if np.any(mask):
                # ≈örednia amplituda w pa≈õmie
                bar_heights[i] = np.mean(fft_magnitude[mask])
        
        # Normalizuj i zastosuj skalƒô logarytmicznƒÖ dla lepszego efektu wizualnego
        bar_heights = np.log10(bar_heights + 1)
        max_height = np.max(bar_heights) if np.max(bar_heights) > 0 else 1
        bar_heights = bar_heights / max_height
        
        # Wyg≈Çad≈∫ (aby uniknƒÖƒá zbyt gwa≈Çtownych zmian)
        bar_heights = np.clip(bar_heights, 0, 1)
        
        return bar_heights
    
    def get_waveform_data(self, start_time, window_duration=0.05, num_points=200):
        """
        Pobierz dane fali d≈∫wiƒôkowej dla obu kana≈Ç√≥w
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            window_duration: D≈Çugo≈õƒá okna w sekundach
            num_points: Liczba punkt√≥w do zwr√≥cenia
            
        Returns:
            Tuple (left_wave, right_wave) - arrays z amplitudami
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(window_duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.left_channel))
        
        if start_sample >= len(self.left_channel):
            return np.zeros(num_points), np.zeros(num_points)
        
        # Pobierz fragmenty
        left_chunk = self.left_channel[start_sample:end_sample]
        right_chunk = self.right_channel[start_sample:end_sample]
        
        if len(left_chunk) == 0:
            return np.zeros(num_points), np.zeros(num_points)
        
        # Resample do num_points
        if len(left_chunk) > num_points:
            # Downsampling
            indices = np.linspace(0, len(left_chunk) - 1, num_points).astype(int)
            left_wave = left_chunk[indices]
            right_wave = right_chunk[indices]
        else:
            # Interpolacja je≈õli za ma≈Ço pr√≥bek
            x_old = np.arange(len(left_chunk))
            x_new = np.linspace(0, len(left_chunk) - 1, num_points)
            left_wave = np.interp(x_new, x_old, left_chunk)
            right_wave = np.interp(x_new, x_old, right_chunk)
        
        return left_wave, right_wave
    
    def extract_vocal_frequencies(self, start_time, window_duration=0.05, num_points=200):
        """
        Ekstraktuj czƒôstotliwo≈õci wokalne (300Hz-3000Hz) z sygna≈Çu stereo
        
        Args:
            start_time: Czas poczƒÖtkowy w sekundach
            window_duration: D≈Çugo≈õƒá okna w sekundach
            num_points: Liczba punkt√≥w do zwr√≥cenia
            
        Returns:
            Array z amplitudami pasma wokalnego
        """
        start_sample = int(start_time * self.sample_rate)
        window_samples = int(window_duration * self.sample_rate)
        end_sample = min(start_sample + window_samples, len(self.left_channel))
        
        if start_sample >= len(self.left_channel):
            return np.zeros(num_points)
        
        # Pobierz fragmenty i u≈õrednij stereo
        left_chunk = self.left_channel[start_sample:end_sample]
        right_chunk = self.right_channel[start_sample:end_sample]
        mono_chunk = (left_chunk + right_chunk) / 2
        
        if len(mono_chunk) == 0:
            return np.zeros(num_points)
        
        # Zastosuj filtr pasmowy dla czƒôstotliwo≈õci wokalnych (300Hz-3000Hz)
        nyquist = self.sample_rate / 2
        low_freq = 300 / nyquist
        high_freq = min(3000 / nyquist, 0.99)
        
        # Projektuj filtr Butterwortha
        from scipy.signal import butter, filtfilt
        b, a = butter(4, [low_freq, high_freq], btype='band')
        
        try:
            vocal_chunk = filtfilt(b, a, mono_chunk)
        except:
            # Je≈õli filtrowanie siƒô nie uda, u≈ºyj surowego sygna≈Çu
            vocal_chunk = mono_chunk
        
        # Resample do num_points
        if len(vocal_chunk) > num_points:
            indices = np.linspace(0, len(vocal_chunk) - 1, num_points).astype(int)
            vocal_wave = vocal_chunk[indices]
        else:
            x_old = np.arange(len(vocal_chunk))
            x_new = np.linspace(0, len(vocal_chunk) - 1, num_points)
            vocal_wave = np.interp(x_new, x_old, vocal_chunk)
        
        return vocal_wave


class VideoGenerator:
    """Klasa do generowania wideo z wizualizacjƒÖ"""
    
    def __init__(self, width, height, fps=30, bars=64, waveform_style='waveform', 
                 left_color=(255, 255, 0), right_color=(0, 255, 0), opacity=0.9,
                 vocal_color=(255, 50, 50), text=None, text_opacity=0.8,
                 watermark=None, watermark_x=10, watermark_y=10):
        """
        Inicjalizacja generatora wideo
        
        Args:
            width: Szeroko≈õƒá wideo
            height: Wysoko≈õƒá wideo
            fps: Klatki na sekundƒô
            bars: Liczba pask√≥w equalizera (lub punkt√≥w dla waveform)
            waveform_style: 'bars' dla equalizera, 'waveform' dla sinusoid
            left_color: Kolor lewego kana≈Çu (R, G, B)
            right_color: Kolor prawego kana≈Çu (R, G, B)
            vocal_color: Kolor wokalu (R, G, B)
            opacity: Przezroczysto≈õƒá wizualizacji (0.0-1.0)
            text: Tekst do wy≈õwietlenia (None = brak)
            text_opacity: Przezroczysto≈õƒá tekstu (0.0-1.0)
            watermark: ≈öcie≈ºka do pliku znaku wodnego (None = brak)
            watermark_x: Pozycja X znaku wodnego w % (0-100)
            watermark_y: Pozycja Y znaku wodnego w % (0-100)
        """
        self.width = width
        self.height = height
        self.fps = fps
        self.bars = bars
        self.waveform_style = waveform_style
        self.left_color = left_color
        self.right_color = right_color
        self.vocal_color = vocal_color
        self.opacity = opacity
        self.text = text
        self.text_opacity = text_opacity
        
        # Historia dla efektu reverb (trailing)
        self.wave_history = []
        
        # Za≈Çaduj font
        self.font = self._load_font()
        
        # Za≈Çaduj font
        self.font = self._load_font()
        
        # Kolory gradientu (od niebieskiego przez zielony do czerwonego)
        if waveform_style == 'bars':
            self.colors = self._generate_gradient_colors()
    
    def _load_font(self):
        """Za≈Çaduj font Arial lub Roboto"""
        font_size = int(self.height * 0.03)  # 3% wysoko≈õci ekranu
        
        # Spr√≥buj r√≥≈ºne fonty
        font_names = [
            'arial.ttf',
            'Arial.ttf',
            'roboto.ttf',
            'Roboto-Regular.ttf',
            'segoeui.ttf',  # Windows fallback
        ]
        
        for font_name in font_names:
            try:
                # Spr√≥buj za≈Çadowaƒá z systemowych czcionek Windows
                font_path = f"C:\\Windows\\Fonts\\{font_name}"
                if os.path.exists(font_path):
                    return ImageFont.truetype(font_path, font_size)
            except:
                continue
        
        # Fallback do domy≈õlnego fontu
        try:
            return ImageFont.truetype("arial.ttf", font_size)
        except:
            return ImageFont.load_default()
        
    def _generate_gradient_colors(self):
        """Generuj gradient kolor√≥w dla pask√≥w"""
        colors = []
        for i in range(self.bars):
            # Gradient: niebieski -> cyan -> zielony -> ≈º√≥≈Çty -> czerwony
            ratio = i / self.bars
            
            if ratio < 0.25:
                # Niebieski do cyan
                r = 0
                g = int((ratio / 0.25) * 255)
                b = 255
            elif ratio < 0.5:
                # Cyan do zielonego
                r = 0
                g = 255
                b = int((1 - (ratio - 0.25) / 0.25) * 255)
            elif ratio < 0.75:
                # Zielony do ≈º√≥≈Çtego
                r = int(((ratio - 0.5) / 0.25) * 255)
                g = 255
                b = 0
            else:
                # ≈ª√≥≈Çty do czerwonego
                r = 255
                g = int((1 - (ratio - 0.75) / 0.25) * 255)
                b = 0
            
            colors.append((r, g, b))
        
        return colors
    
    def create_frame(self, bar_heights=None, smoothed_heights=None, 
                    left_wave=None, right_wave=None, vocal_wave=None, background=None):
        """
        Utw√≥rz pojedynczƒÖ klatkƒô z wizualizacjƒÖ
        
        Args:
            bar_heights: Array z wysoko≈õciami pask√≥w (0-1) - dla stylu 'bars'
            smoothed_heights: Poprzednie wysoko≈õci dla wyg≈Çadzania
            left_wave: Array z amplitudami lewego kana≈Çu - dla stylu 'waveform'
            right_wave: Array z amplitudami prawego kana≈Çu - dla stylu 'waveform'
            vocal_wave: Array z amplitudami wokalu - dla stylu 'waveform'
            background: PIL Image z t≈Çem (opcjonalne)
            
        Returns:
            numpy array z wizualizacjƒÖ
        """
        # U≈ºyj t≈Ça lub utw√≥rz czarne
        if background is not None:
            img = background.copy()
        else:
            img = Image.new('RGB', (self.width, self.height), color=(0, 0, 0))
        
        # Utw√≥rz warstwƒô z wizualizacjƒÖ
        overlay = Image.new('RGBA', (self.width, self.height), color=(0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        if self.waveform_style == 'waveform' and left_wave is not None and right_wave is not None:
            # Rysuj sinusoidy
            self._draw_waveforms(draw, left_wave, right_wave, vocal_wave)
        elif self.waveform_style == 'bars' and bar_heights is not None:
            # Rysuj equalizera (stary spos√≥b)
            self._draw_bars(draw, bar_heights, smoothed_heights)
        
        # Zastosuj blur do wizualizacji
        overlay = overlay.filter(ImageFilter.GaussianBlur(radius=3))
        
        # Na≈Ç√≥≈º wizualizacjƒô na t≈Ço
        img = img.convert('RGBA')
        img = Image.alpha_composite(img, overlay)
        
        # Dodaj tekst je≈õli jest ustawiony
        if self.text:
            self._draw_text(img)
        
        img = img.convert('RGB')
        
        return np.array(img)
    
    def _draw_text(self, img):
        """Rysuj tekst w prawym dolnym rogu"""
        draw = ImageDraw.Draw(img)
        
        # Oblicz pozycjƒô (1% marginesu)
        margin_x = int(self.width * 0.01)
        margin_y = int(self.height * 0.01)
        
        # Pobierz rozmiar tekstu
        try:
            bbox = draw.textbbox((0, 0), self.text, font=self.font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
        except:
            # Fallback dla starszych wersji Pillow
            text_width, text_height = draw.textsize(self.text, font=self.font)
        
        # Pozycja: prawy dolny r√≥g z marginesem
        x = self.width - text_width - margin_x
        y = self.height - text_height - margin_y
        
        # Rysuj tekst z cieniem dla lepszej czytelno≈õci
        shadow_offset = 2
        shadow_color = (0, 0, 0, int(255 * self.text_opacity))
        text_color = (255, 255, 255, int(255 * self.text_opacity))
        
        # Cie≈Ñ
        draw.text((x + shadow_offset, y + shadow_offset), self.text, 
                 font=self.font, fill=shadow_color)
        # Tekst
        draw.text((x, y), self.text, font=self.font, fill=text_color)
    
    def _draw_waveforms(self, draw, left_wave, right_wave, vocal_wave=None):
        """
        Rysuj trzy sinusoidy (≈º√≥≈Çta dla lewego, zielona dla prawego, czerwona dla wokalu) na ≈õrodku ekranu z efektem reverb
        
        Args:
            draw: ImageDraw object
            left_wave: Array z amplitudami lewego kana≈Çu
            right_wave: Array z amplitudami prawego kana≈Çu
            vocal_wave: Array z amplitudami wokalu (opcjonalne)
        """
        # Parametry
        center_y = self.height / 2
        amplitude_scale = self.height * 0.35  # 35% wysoko≈õci dla amplitudy
        line_width = 3
        
        # Lewy kana≈Ç - ≈º√≥≈Çty (na ≈õrodku)
        points_left = []
        for i, amp in enumerate(left_wave):
            x = (i / len(left_wave)) * self.width
            y = center_y + (amp * amplitude_scale)
            points_left.append((x, y))
        
        # Prawy kana≈Ç - zielony (na ≈õrodku - na sobie)
        points_right = []
        for i, amp in enumerate(right_wave):
            x = (i / len(right_wave)) * self.width
            y = center_y + (amp * amplitude_scale)
            points_right.append((x, y))
        
        # Wokal - czerwony (na ≈õrodku, je≈õli jest)
        points_vocal = []
        if vocal_wave is not None:
            for i, amp in enumerate(vocal_wave):
                x = (i / len(vocal_wave)) * self.width
                y = center_y + (amp * amplitude_scale * 0.8)  # Trochƒô mniejsza amplituda
                points_vocal.append((x, y))
        
        # Dodaj do historii dla efektu reverb
        self.wave_history.append((points_left, points_right, points_vocal))
        
        # Zachowaj tylko ostatnie N klatek dla efektu trailing
        max_history = 5
        if len(self.wave_history) > max_history:
            self.wave_history.pop(0)
        
        # Rysuj trailing (starsze fale z mniejszƒÖ opacity)
        for idx, history_item in enumerate(self.wave_history[:-1]):
            # Rozpakowuj historiƒô (mo≈ºe byƒá 2 lub 3 elementy)
            if len(history_item) == 3:
                old_left, old_right, old_vocal = history_item
            else:
                old_left, old_right = history_item
                old_vocal = []
            
            # Oblicz opacity dla starszych klatek (efekt zanikania)
            age_factor = (idx + 1) / len(self.wave_history)
            trail_opacity = self.opacity * age_factor * 0.3  # S≈Çabsze dla trailing
            
            # Rysuj trailing lewego kana≈Çu
            if len(old_left) > 1:
                for i in range(len(old_left) - 1):
                    color_with_alpha = self.left_color + (int(255 * trail_opacity),)
                    draw.line([old_left[i], old_left[i + 1]], 
                             fill=color_with_alpha, width=line_width)
            
            # Rysuj trailing prawego kana≈Çu
            if len(old_right) > 1:
                for i in range(len(old_right) - 1):
                    color_with_alpha = self.right_color + (int(255 * trail_opacity),)
                    draw.line([old_right[i], old_right[i + 1]], 
                             fill=color_with_alpha, width=line_width)
            
            # Rysuj trailing wokalu
            if len(old_vocal) > 1:
                for i in range(len(old_vocal) - 1):
                    color_with_alpha = self.vocal_color + (int(255 * trail_opacity),)
                    draw.line([old_vocal[i], old_vocal[i + 1]], 
                             fill=color_with_alpha, width=line_width)
        
        # Rysuj aktualnƒÖ falƒô (pe≈Çna opacity)
        if len(points_left) > 1:
            for i in range(len(points_left) - 1):
                color_with_alpha = self.left_color + (int(255 * self.opacity),)
                draw.line([points_left[i], points_left[i + 1]], 
                         fill=color_with_alpha, width=line_width)
        
        if len(points_right) > 1:
            for i in range(len(points_right) - 1):
                color_with_alpha = self.right_color + (int(255 * self.opacity),)
                draw.line([points_right[i], points_right[i + 1]], 
                         fill=color_with_alpha, width=line_width)
        
        # Rysuj aktualnƒÖ falƒô wokalu (na wierzchu)
        if len(points_vocal) > 1:
            for i in range(len(points_vocal) - 1):
                color_with_alpha = self.vocal_color + (int(255 * self.opacity),)
                draw.line([points_vocal[i], points_vocal[i + 1]], 
                         fill=color_with_alpha, width=line_width)
    
    def _draw_bars(self, draw, bar_heights, smoothed_heights):
        """
        Rysuj equalizera (paski)
        
        Args:
            draw: ImageDraw object  
            bar_heights: Array z wysoko≈õciami pask√≥w (0-1)
            smoothed_heights: Poprzednie wysoko≈õci dla wyg≈Çadzania
        """
        # Wyg≈Çad≈∫ przej≈õcia miƒôdzy klatkami
        if smoothed_heights is not None:
            bar_heights = 0.7 * bar_heights + 0.3 * smoothed_heights
        
        # Parametry pask√≥w
        bar_width = self.width / self.bars
        max_bar_height = self.height * 0.8
        base_y = self.height * 0.9
        
        # Rysuj paski
        for i, height in enumerate(bar_heights):
            x = i * bar_width
            bar_h = height * max_bar_height
            y = base_y - bar_h
            
            # G≈Ç√≥wny pasek z alpha
            color = self.colors[i]
            color_with_alpha = color + (int(255 * 0.6),)
            
            # Konwertuj do int dla rectangle
            x1, y1 = int(x + 1), int(y)
            x2, y2 = int(x + bar_width - 1), int(base_y)
            
            # Rysuj prostokƒÖt
            for yi in range(y1, y2):
                draw.line([x1, yi, x2, yi], fill=color_with_alpha)


def process_batch(batch_dir, args):
    """
    Przetwarzanie wsadowe katalog√≥w
    
    Args:
        batch_dir: Katalog zawierajƒÖcy podkatalogi z plikami WAV i obrazkami
        args: Argumenty z parsera
    """
    if not os.path.isdir(batch_dir):
        print(f"‚ùå {batch_dir} nie jest katalogiem")
        return
    
    # Parse kolor√≥w
    left_color = tuple(map(int, args.left_color.split(',')))
    right_color = tuple(map(int, args.right_color.split(',')))
    
    print(f"üîÑ Tryb batch: przetwarzam katalog {batch_dir}")
    print("=" * 70)
    
    # Szukaj podkatalog√≥w
    subdirs = [d for d in os.listdir(batch_dir) 
               if os.path.isdir(os.path.join(batch_dir, d))]
    
    if not subdirs:
        print(f"‚ùå Brak podkatalog√≥w w {batch_dir}")
        return
    
    total = len(subdirs)
    for idx, subdir in enumerate(subdirs, 1):
        subdir_path = os.path.join(batch_dir, subdir)
        print(f"\n[{idx}/{total}] üìÅ Przetwarzam: {subdir}")
        print("-" * 70)
        
        # Znajd≈∫ plik WAV
        wav_files = glob.glob(os.path.join(subdir_path, "*.wav")) + \
                   glob.glob(os.path.join(subdir_path, "*.WAV"))
        
        if not wav_files:
            print(f"‚ö†Ô∏è  Brak pliku WAV w {subdir}, pomijam...")
            continue
        
        wav_file = wav_files[0]  # U≈ºyj pierwszego znalezionego
        
        # Sprawd≈∫ czy sƒÖ obrazki w podkatalogu
        image_files = []
        for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:
            image_files.extend(glob.glob(os.path.join(subdir_path, ext)))
        
        # U≈ºyj katalogu jako t≈Ço je≈õli sƒÖ obrazki, w przeciwnym razie None
        background = subdir_path if image_files else None
        
        # Wygeneruj nazwƒô pliku wyj≈õciowego
        base_name = os.path.splitext(os.path.basename(wav_file))[0]
        output_file = os.path.join(subdir_path, f"{base_name}.mp4")
        
        try:
            create_video_from_wav(
                wav_file,
                output_file,
                resolution=args.resolution,
                audio_bitrate=args.audio_bitrate,
                fps=args.fps,
                bars=args.bars,
                background=background,
                waveform_style=args.style,
                left_color=left_color,
                right_color=right_color,
                opacity=args.opacity,
                text=args.text,
                text_opacity=args.text_opacity,
                watermark=args.watermark,
                watermark_x=args.watermark_x,
                watermark_y=args.watermark_y,
                test_length=args.test_length
            )
            print(f"‚úÖ Uko≈Ñczono: {output_file}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd dla {subdir}: {e}")
            continue
    
    print("\n" + "=" * 70)
    print(f"üéâ Batch processing zako≈Ñczony! Przetworzono {total} katalog√≥w.")


def create_video_from_wav(input_wav, output_mp4, resolution="1920x1080", 
                         audio_bitrate="320k", fps=30, bars=500,
                         background=None, waveform_style='waveform',
                         left_color=(255, 255, 0), right_color=(0, 255, 0),
                         opacity=0.9, text=None, text_opacity=0.8,
                         watermark=None, watermark_x=10, watermark_y=10,
                         test_length=None):
    """
    G≈Ç√≥wna funkcja konwertujƒÖca WAV do MP4 z wizualizacjƒÖ
    
    Args:
        input_wav: ≈öcie≈ºka do pliku WAV
        output_mp4: ≈öcie≈ºka do pliku MP4 wyj≈õciowego
        resolution: Rozdzielczo≈õƒá w formacie "WIDTHxHEIGHT"
        audio_bitrate: Bitrate audio (np. "320k", "192k")
        fps: Klatki na sekundƒô
        bars: Liczba pask√≥w equalizera/punkt√≥w fali
        background: ≈öcie≈ºka do obrazka/katalogu z t≈Çem
        waveform_style: 'waveform' dla sinusoid, 'bars' dla equalizera
        left_color: Kolor lewego kana≈Çu (R, G, B)
        right_color: Kolor prawego kana≈Çu (R, G, B)
        opacity: Przezroczysto≈õƒá wizualizacji (0.0-1.0)
        text: Tekst do wy≈õwietlenia (None = brak)
        text_opacity: Przezroczysto≈õƒá tekstu (0.0-1.0)
    """
    print(f"üìÅ Wczytujƒô plik: {input_wav}")
    
    # Parse resolution
    width, height = map(int, resolution.lower().split('x'))
    print(f"üì∫ Rozdzielczo≈õƒá: {width}x{height}")
    print(f"üéµ Bitrate audio: {audio_bitrate}")
    print(f"üé¨ FPS: {fps}")
    print(f"üìä Styl wizualizacji: {waveform_style}")
    if background:
        print(f"üñºÔ∏è  T≈Ço: {background}")
    
    # Inicjalizuj analizator audio
    visualizer = AudioVisualizer(input_wav, num_bars=bars)
    
    # Tryb testowy - skr√≥ƒá d≈Çugo≈õƒá
    original_duration = visualizer.duration
    if test_length is not None:
        visualizer.duration = original_duration * (test_length / 100)
        print(f"‚ö° TRYB TESTOWY: {test_length}% pliku ({visualizer.duration:.2f}s z {original_duration:.2f}s)")
    
    print(f"‚è±Ô∏è  D≈Çugo≈õƒá: {visualizer.duration:.2f} sekund")
    print(f"üîä Format: {'Stereo' if visualizer.is_stereo else 'Mono'}")
    
    # Inicjalizuj generator wideo
    video_gen = VideoGenerator(width, height, fps, bars, waveform_style, 
                              left_color, right_color, opacity,
                              vocal_color=(255, 50, 50), text=text, text_opacity=text_opacity,
                              watermark=watermark, watermark_x=watermark_x, watermark_y=watermark_y)
    
    # Inicjalizuj manager t≈Ça
    bg_manager = BackgroundManager(background, width, height, visualizer.duration)
    
    # Stan dla wyg≈Çadzania animacji
    previous_heights = np.zeros(bars)
    previous_left_wave = np.zeros(bars)
    previous_right_wave = np.zeros(bars)
    previous_vocal_wave = np.zeros(bars)
    
    def make_frame(t):
        """Funkcja generujƒÖca klatkƒô dla czasu t"""
        nonlocal previous_heights, previous_left_wave, previous_right_wave, previous_vocal_wave
        
        # Pobierz t≈Ço
        bg = bg_manager.get_frame(t)
        
        if waveform_style == 'waveform':
            # Pobierz dane fali dla obu kana≈Ç√≥w
            left_wave, right_wave = visualizer.get_waveform_data(t, num_points=bars)
            
            # Wyg≈Çadzanie
            left_wave = 0.7 * left_wave + 0.3 * previous_left_wave
            right_wave = 0.7 * right_wave + 0.3 * previous_right_wave
            
            # Ekstraktuj wokal
            vocal_wave = visualizer.extract_vocal_frequencies(t, num_points=bars)
            vocal_wave = 0.7 * vocal_wave + 0.3 * previous_vocal_wave
            
            # Utw√≥rz klatkƒô
            frame = video_gen.create_frame(
                left_wave=left_wave,
                right_wave=right_wave,
                vocal_wave=vocal_wave,
                background=bg
            )
            
            # Zapamiƒôtaj
            previous_left_wave = left_wave
            previous_right_wave = right_wave
            previous_vocal_wave = vocal_wave
        else:
            # Styl equalizera (bars)
            bar_heights = visualizer.get_frequency_spectrum(t)
            
            # Utw√≥rz klatkƒô
            frame = video_gen.create_frame(
                bar_heights=bar_heights,
                smoothed_heights=previous_heights,
                background=bg
            )
            
            # Zapamiƒôtaj
            previous_heights = bar_heights
        
        return frame
    
    print("üé® Generujƒô wizualizacjƒô...")
    
    # Utw√≥rz klip wideo
    video_clip = VideoClip(make_frame, duration=visualizer.duration)
    video_clip = video_clip.with_fps(fps)
    
    # Wczytaj audio
    audio_clip = AudioFileClip(input_wav)
    
    # Po≈ÇƒÖcz wideo z audio
    final_clip = video_clip.with_audio(audio_clip)
    
    print(f"üíæ Zapisujƒô do: {output_mp4}")
    
    # Zapisz jako MP4 z dobrƒÖ jako≈õciƒÖ
    # moviepy automatycznie zachowa metadane audio z oryginalnego pliku WAV
    final_clip.write_videofile(
        output_mp4,
        codec='libx264',
        audio_codec='aac',
        audio_bitrate=audio_bitrate,
        fps=fps,
        preset='slow',  # Lepsza jako≈õƒá, wolniejsze kodowanie
        bitrate='8000k',  # Wysokie bitrate wideo dla dobrej jako≈õci
        # Zachowaj metadane audio
        ffmpeg_params=['-map_metadata', '0']
    )
    
    print("‚úÖ Gotowe!")
    print(f"üì¶ Plik zapisany: {output_mp4}")


def main():
    """G≈Ç√≥wna funkcja programu"""
    parser = argparse.ArgumentParser(
        description='Konwertuj WAV do MP4 z wizualnƒÖ wizualizacjƒÖ audio',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Przyk≈Çady u≈ºycia:
  # Podstawowe z tekstem
  python main.py song.wav output.mp4 --text "My Song Title"
  
  # Z t≈Çem i znakiem wodnym
  python main.py song.wav output.mp4 --background photo.jpg --watermark logo.png
  
  # Test pierwszych 10%% (szybkie sprawdzenie)
  python main.py song.wav test.mp4 --test-length 10
  
  # Pe≈Çna konfiguracja
  python main.py song.wav output.mp4 --background ./images/ --text "Song 2025" --watermark logo.png --watermark-x 5 --watermark-y 5
  
  # Tryb batch
  python main.py batch-folder dummy.mp4 --batch
        """
    )
    
    parser.add_argument('input', help='Plik WAV wej≈õciowy')
    parser.add_argument('output', help='Plik MP4 wyj≈õciowy')
    parser.add_argument('--resolution', default='1920x1080',
                       help='Rozdzielczo≈õƒá wideo (domy≈õlnie: 1920x1080)')
    parser.add_argument('--audio-bitrate', default='320k',
                       help='Bitrate audio (domy≈õlnie: 320k)')
    parser.add_argument('--fps', type=int, default=30,
                       help='Klatki na sekundƒô (domy≈õlnie: 30)')
    parser.add_argument('--bars', type=int, default=500,
                       help='Liczba punkt√≥w wizualizacji (domy≈õlnie: 500 dla lepszej rozdzielczo≈õci)')
    parser.add_argument('--background', default=None,
                       help='≈öcie≈ºka do obrazka lub katalogu z obrazkami dla t≈Ça')
    parser.add_argument('--style', default='waveform', choices=['waveform', 'bars'],
                       help='Styl wizualizacji: waveform (sinusoidy) lub bars (equalizera)')
    parser.add_argument('--left-color', default='255,255,0',
                       help='Kolor lewego kana≈Çu w formacie R,G,B (domy≈õlnie: 255,255,0 - ≈º√≥≈Çty)')
    parser.add_argument('--right-color', default='0,255,0',
                       help='Kolor prawego kana≈Çu w formacie R,G,B (domy≈õlnie: 0,255,0 - zielony)')
    parser.add_argument('--opacity', type=float, default=0.9,
                       help='Przezroczysto≈õƒá wizualizacji 0.0-1.0 (domy≈õlnie: 0.9)')
    parser.add_argument('--text', default=None,
                       help='Tekst do wy≈õwietlenia w prawym dolnym rogu (zawsze CAPS)')
    parser.add_argument('--text-opacity', type=float, default=0.8,
                       help='Przezroczysto≈õƒá tekstu 0.0-1.0 (domy≈õlnie: 0.8)')
    parser.add_argument('--watermark', default=None,
                       help='≈öcie≈ºka do pliku znaku wodnego (PNG/JPG z alpha channel)')
    parser.add_argument('--watermark-x', type=float, default=10,
                       help='Pozycja X znaku wodnego w %% od lewej (domy≈õlnie: 10)')
    parser.add_argument('--watermark-y', type=float, default=10,
                       help='Pozycja Y znaku wodnego w %% od g√≥ry (domy≈õlnie: 10)')
    parser.add_argument('--test-length', type=float, default=None,
                       help='Renderuj tylko X%% pliku dla szybkich test√≥w (np. 10 = pierwsze 10%%)')
    parser.add_argument('--batch', action='store_true',
                       help='Tryb batch - przetwarzaj katalogi z podkatalogami zawierajƒÖcymi WAV+obrazki')
    
    args = parser.parse_args()
    
    # Parse kolor√≥w
    try:
        left_color = tuple(map(int, args.left_color.split(',')))
        right_color = tuple(map(int, args.right_color.split(',')))
        
        if len(left_color) != 3 or len(right_color) != 3:
            raise ValueError("Kolory muszƒÖ mieƒá 3 sk≈Çadowe (R,G,B)")
    except ValueError as e:
        print(f"‚ùå B≈ÇƒÖd parsowania kolor√≥w: {e}", file=sys.stderr)
        sys.exit(1)
    
    try:
        if args.batch:
            # Tryb batch processing
            process_batch(args.input, args)
        else:
            # Pojedynczy plik
            create_video_from_wav(
                args.input,
                args.output,
                resolution=args.resolution,
                audio_bitrate=args.audio_bitrate,
                fps=args.fps,
                bars=args.bars,
                background=args.background,
                waveform_style=args.style,
                left_color=left_color,
                right_color=right_color,
                opacity=args.opacity,
                text=args.text,
                text_opacity=args.text_opacity,
                watermark=args.watermark,
                watermark_x=args.watermark_x,
                watermark_y=args.watermark_y,
                test_length=args.test_length
            )
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
